{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnning Llama on the prompts\n",
    "\n",
    "Now we can test the full loop, before writing a job manifest, we will:\n",
    "1. load Llama into memory from our pickled weights\n",
    "2. normalised the data chunk to get prompts\n",
    "3. run the LLM on our promps\n",
    "4. save the hidden activations to disk\n",
    "\n",
    "5. (optional) implement caching for the definitions, since they may be shared between multiple words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7f43c066b170>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "tok = Tokenizer(tok_path)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-12 12:54:03,336:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Data/ModelWeights/llama_jax_weights.pkl\", \"rb\") as f:\n",
    "    params_jax_loaded = pickle.load(f)\n",
    "\n",
    "n_heads = 32\n",
    "n_kv_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Processed/SemCoreChunks/chunk_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def fix_whitespace(text):\n",
    "    # Remove spaces before punctuation\n",
    "    text = re.sub(r'\\s+([?.!,;:])', r'\\1', text)\n",
    "    # Ensure space after punctuation if followed by a word (except for some cases like commas within numbers)\n",
    "    text = re.sub(r'([?.!;:])(?=[^\\s])', r'\\1 ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "type series = pd.core.series.Series\n",
    "\n",
    "def prep_row(row: series) -> series:\n",
    "    row[\"all_defs\"] = str.split(row.definitions, '|')\n",
    "    row[\"sentence_str\"]  = ' ' + fix_whitespace(' '.join(str.split(row.sentence, '|')))  \n",
    "    row[\"sentence_toks\"] = tok.encode(row[\"sentence_str\"], bos = False, eos = False)\n",
    "\n",
    "    remaining = row[\"sentence_str\"]\n",
    "\n",
    "    word_start_index = 0\n",
    "\n",
    "    for i in range(row[\"word_loc\"]):\n",
    "        word = tok.decode([row[\"sentence_toks\"][i]])\n",
    "        word_start_index += remaining.find(word)\n",
    "        word_start_index += len(word)\n",
    "        remaining = row[\"sentence_str\"][word_start_index:]\n",
    "\n",
    "\n",
    "    row[\"word_start_index\"] = word_start_index\n",
    "\n",
    "    word_tok_index = 0\n",
    "    i = 0\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        word = tok.decode([row[\"sentence_toks\"][word_tok_index]])\n",
    "        \n",
    "        i += len(word)\n",
    "        \n",
    "        if i > word_start_index:\n",
    "            break\n",
    "        \n",
    "        word_tok_index  += 1\n",
    "\n",
    "    row[\"word_tok_index\"] = word_tok_index\n",
    "\n",
    "    encode = lambda  text : tok.encode(text, bos = False, eos = False)\n",
    "    if len(encode(\" \" + row[\"word\"])) == len(encode(\" \" + row[\"word\"] + \":\")):\n",
    "        return Exception(\"colon was absorbed - changing token embedding!\")\n",
    "    \n",
    "    defs = row[\"definitions\"].split('|')\n",
    "\n",
    "    row[\"definition_prompts\"] = [\" \" + row[\"word\"] + \": \" + d for d in defs]\n",
    "    row[\"definition_toks\"] = [encode(def_prompt) for def_prompt in row[\"definition_prompts\"]]\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 103.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_new = df.progress_apply(prep_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>How|long|has|it|been|since|you|reviewed|the|ob...</td>\n",
       "      <td>1</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>primarily temporal sense; being or indicating ...</td>\n",
       "      <td>desire strongly or persistently|primarily temp...</td>\n",
       "      <td>[desire strongly or persistently, primarily te...</td>\n",
       "      <td>How long has it been since you reviewed the o...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[ long: desire strongly or persistently,  long...</td>\n",
       "      <td>[[1317, 25, 12876, 16917, 477, 23135, 4501], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>been</td>\n",
       "      <td>How|long|has|it|been|since|you|reviewed|the|ob...</td>\n",
       "      <td>4</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "      <td>have the quality of being; (copula, used with ...</td>\n",
       "      <td>a light strong brittle grey toxic bivalent met...</td>\n",
       "      <td>[a light strong brittle grey toxic bivalent me...</td>\n",
       "      <td>How long has it been since you reviewed the o...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[ been: a light strong brittle grey toxic biva...</td>\n",
       "      <td>[[1027, 25, 264, 3177, 3831, 95749, 20366, 215...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word                                           sentence  word_loc  \\\n",
       "0  long  How|long|has|it|been|since|you|reviewed|the|ob...         1   \n",
       "1  been  How|long|has|it|been|since|you|reviewed|the|ob...         4   \n",
       "\n",
       "          wordnet                                         definition  \\\n",
       "0  long%3:00:02::  primarily temporal sense; being or indicating ...   \n",
       "1    be%2:42:03::  have the quality of being; (copula, used with ...   \n",
       "\n",
       "                                         definitions  \\\n",
       "0  desire strongly or persistently|primarily temp...   \n",
       "1  a light strong brittle grey toxic bivalent met...   \n",
       "\n",
       "                                            all_defs  \\\n",
       "0  [desire strongly or persistently, primarily te...   \n",
       "1  [a light strong brittle grey toxic bivalent me...   \n",
       "\n",
       "                                        sentence_str  \\\n",
       "0   How long has it been since you reviewed the o...   \n",
       "1   How long has it been since you reviewed the o...   \n",
       "\n",
       "                                       sentence_toks  word_start_index  \\\n",
       "0  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                 4   \n",
       "1  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                16   \n",
       "\n",
       "   word_tok_index                                 definition_prompts  \\\n",
       "0               1  [ long: desire strongly or persistently,  long...   \n",
       "1               4  [ been: a light strong brittle grey toxic biva...   \n",
       "\n",
       "                                     definition_toks  \n",
       "0  [[1317, 25, 12876, 16917, 477, 23135, 4501], [...  \n",
       "1  [[1027, 25, 264, 3177, 3831, 95749, 20366, 215...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_jax_loaded[\"freqs_cis\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from LLM.llama_jax.model import reporting_transformer\n",
    "\n",
    "\n",
    "jitted_transformer = jax.jit(reporting_transformer, static_argnames=[\"n_heads\", \"n_kv_heads\"])\n",
    "\n",
    "\n",
    "max_tok_len = 256\n",
    "\n",
    "params_jax_loaded[\"freqs_cis\"] = params_jax_loaded[\"freqs_cis\"][0:max_tok_len, :]\n",
    "\n",
    "def pad_toks(toks: List[int]):\n",
    "    to_pad = max_tok_len - len(toks)\n",
    "\n",
    "    if (to_pad < 0):\n",
    "        ret = toks[0:to_pad]\n",
    "    else:\n",
    "        ret = toks + [0] * to_pad\n",
    "\n",
    "    mask = [0 if tok != 0 else -jnp.inf for tok in ret]\n",
    "\n",
    "    return jnp.array(ret)[None, :], jnp.array(mask)\n",
    "\n",
    "\n",
    "def get_activations(toks: List[int]):\n",
    "    padded_toks, mask = pad_toks(toks)\n",
    "    return jitted_transformer(padded_toks, params_jax_loaded, mask, n_heads, n_kv_heads)\n",
    "\n",
    "\n",
    "def LLM_process_row(row: series) -> series:\n",
    "    sentence_acts = get_activations(row.sentence_toks)\n",
    "    def_acts     = [get_activations(def_toks) for def_toks in row.definition_toks]\n",
    "\n",
    "    # TODO - if the data volume is too large, then only store the activations\n",
    "    # near the word itself\n",
    "    row[\"sentence_activations\"] = sentence_acts\n",
    "    row[\"definition_activates\"] = def_acts\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_small = df_new.head(1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:05<00:00, 65.29s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_new_small = df_new_small.progress_apply(LLM_process_row, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "      <th>sentence_activations</th>\n",
       "      <th>definition_activates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>How|long|has|it|been|since|you|reviewed|the|ob...</td>\n",
       "      <td>1</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>primarily temporal sense; being or indicating ...</td>\n",
       "      <td>desire strongly or persistently|primarily temp...</td>\n",
       "      <td>[desire strongly or persistently, primarily te...</td>\n",
       "      <td>How long has it been since you reviewed the o...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[ long: desire strongly or persistently,  long...</td>\n",
       "      <td>[[1317, 25, 12876, 16917, 477, 23135, 4501], [...</td>\n",
       "      <td>[[[[0.0119629 -0.00976562 -0.0106201 ... -0.01...</td>\n",
       "      <td>[[[[[0.000976562 0.0205078 0.0634766 ... -0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word                                           sentence  word_loc  \\\n",
       "0  long  How|long|has|it|been|since|you|reviewed|the|ob...         1   \n",
       "\n",
       "          wordnet                                         definition  \\\n",
       "0  long%3:00:02::  primarily temporal sense; being or indicating ...   \n",
       "\n",
       "                                         definitions  \\\n",
       "0  desire strongly or persistently|primarily temp...   \n",
       "\n",
       "                                            all_defs  \\\n",
       "0  [desire strongly or persistently, primarily te...   \n",
       "\n",
       "                                        sentence_str  \\\n",
       "0   How long has it been since you reviewed the o...   \n",
       "\n",
       "                                       sentence_toks  word_start_index  \\\n",
       "0  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                 4   \n",
       "\n",
       "   word_tok_index                                 definition_prompts  \\\n",
       "0               1  [ long: desire strongly or persistently,  long...   \n",
       "\n",
       "                                     definition_toks  \\\n",
       "0  [[1317, 25, 12876, 16917, 477, 23135, 4501], [...   \n",
       "\n",
       "                                sentence_activations  \\\n",
       "0  [[[[0.0119629 -0.00976562 -0.0106201 ... -0.01...   \n",
       "\n",
       "                                definition_activates  \n",
       "0  [[[[[0.000976562 0.0205078 0.0634766 ... -0.01...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 2048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_small.iloc[0].sentence_activations[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data volume\n",
    "\n",
    "We are seeing roughly 1Mb of data per layer, so 16Mb per sentence/definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"num_defs\"] = df_new[\"definition_toks\"].apply(lambda ds : len(ds))\n",
    "df_new[\"num_defs\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have 1000 setences + definitions an so will be generating 16Gb of data per chunk, and thus would need ~30Tb of storage to process it all!\n",
    "\n",
    "Lets cut down the volume, first lets see what is actually a reasonable max token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"number_sentence_toks\"] = df_new[\"sentence_toks\"].apply(lambda toks : len(toks))\n",
    "df_new[\"number_def_toks\"] = df_new[\"definition_toks\"].apply(lambda defs_toks : [len(def_toks) for def_toks in defs_toks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean      20.710000\n",
       "std        8.210974\n",
       "min        6.000000\n",
       "25%       13.000000\n",
       "50%       19.000000\n",
       "75%       27.000000\n",
       "max       35.000000\n",
       "Name: number_sentence_toks, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"number_sentence_toks\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Third': 13.5, 'Median': 9.0, 'Mean': 10.783902976846747, 'Max': 42}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_values = [num for sublist in df_new['number_def_toks'] for num in sublist]\n",
    "\n",
    "# Compute statistics\n",
    "third = np.percentile(all_values, 75)\n",
    "median = np.median(all_values)\n",
    "mean = np.mean(all_values)\n",
    "max_val = np.max(all_values)\n",
    "\n",
    "# Print results\n",
    "stats = {\n",
    "    \"Third\" : third,\n",
    "    \"Median\": median,\n",
    "    \"Mean\": mean,\n",
    "    \"Max\": max_val,\n",
    "}\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limiting the inputs to 32 tokens looks like it would still keep most of the information we need, the definitions can be clipped from the ends... for the sentences we should clip outwards from the word, luckily we have the word token indices for that\n",
    "\n",
    "That will yield a 3-fold (8x) reduction in data volume to roughly 4Tb. However we will not necessarily process all the data, and 500Mb per chunk seems more manageable for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_definition_token_list(def_toks: List[List[int]], max_tok_len = 32) -> List[List[int]]:\n",
    "    ret = []\n",
    "    for _, lst in enumerate(def_toks):\n",
    "        if len(lst) <= max_tok_len:\n",
    "            ret.append(lst)\n",
    "        else:\n",
    "            ret.append(lst[0:max_tok_len])\n",
    "    return ret\n",
    "\n",
    "def clip_sentence_token_list(row: series, max_tok_len = 32) -> series:\n",
    "\n",
    "    range = (0,0)\n",
    "    new_index = row.word_tok_index\n",
    "\n",
    "    if len(row.sentence_toks) <= max_tok_len:\n",
    "        range = (0,None)\n",
    "    elif (row.word_tok_index <= max_tok_len // 2):\n",
    "        range = (0,max_tok_len)\n",
    "    elif (len(row.sentence_toks) - row.word_tok_index <= max_tok_len // 2):\n",
    "        range = (-max_tok_len, None)\n",
    "        new_index -= (len(row.sentence_toks) - max_tok_len) # this much has been removed\n",
    "    else:\n",
    "        delta = max_tok_len // 2 \n",
    "        mod   = max_tok_len %  2\n",
    "        mid = row.word_tok_index\n",
    "        range = (mid - delta, mid + delta + mod)\n",
    "        new_index -= (mid - delta)\n",
    "\n",
    "    row[\"clipped_sentence_toks\"] = row.sentence_toks[range[0]:range[1]]\n",
    "    row[\"clipped_word_tok_index\"] = new_index\n",
    "\n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# tests a la chatgpt\n",
    "\n",
    "# 1) Sentence length <= max_tok_len\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": [\"I\", \"love\", \"Python\"],\n",
    "    \"word_tok_index\": 1\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=5)\n",
    "assert row[\"clipped_sentence_toks\"] == [\"I\", \"love\", \"Python\"]\n",
    "assert row[\"clipped_word_tok_index\"] == 1\n",
    "\n",
    "# 2) Focus token near the beginning\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": [\"Token\"] * 20,\n",
    "    \"word_tok_index\": 2\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=6)\n",
    "# Should keep from index 0 up to index 6, word_tok_index should remain 2\n",
    "assert len(row[\"clipped_sentence_toks\"]) == 6\n",
    "assert row[\"clipped_word_tok_index\"] == 2\n",
    "\n",
    "# 3) Focus token near the end\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": [\"Token\"] * 20,\n",
    "    \"word_tok_index\": 18\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=5)\n",
    "# Should keep last 5 tokens\n",
    "# Original indices: 15,16,17,18,19\n",
    "# 'word_tok_index' = 18 => new_index = 18 - 15 = 3\n",
    "assert len(row[\"clipped_sentence_toks\"]) == 5\n",
    "assert row[\"clipped_word_tok_index\"] == 3\n",
    "\n",
    "# 4) Focus token somewhere in the middle\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": list(range(30)),  # Just use integer tokens for clarity\n",
    "    \"word_tok_index\": 15\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=6)\n",
    "# max_tok_len=6 => delta=3 => mod=0\n",
    "# start=15-3=12, end=15+3=18 => slice is [12,13,14,15,16,17]\n",
    "# new_index = 15 - 12 = 3\n",
    "assert row[\"clipped_sentence_toks\"] == [12, 13, 14, 15, 16, 17]\n",
    "assert row[\"clipped_word_tok_index\"] == 3\n",
    "\n",
    "# 5) Edge case: focus token exactly on boundary (like index=10, max_tok_len=6)\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": list(range(20)),\n",
    "    \"word_tok_index\": 10\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=6)\n",
    "# delta=3, mod=0 => slice [7..13) => [7,8,9,10,11,12], length=6\n",
    "# new_index = 10 - 7 = 3\n",
    "assert len(row[\"clipped_sentence_toks\"]) == 6\n",
    "assert row[\"clipped_word_tok_index\"] == 3\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"clipped_definition_toks\"] = df_new[\"definition_toks\"].apply(clip_definition_token_list)\n",
    "df_new = df_new.apply(clip_sentence_token_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "      <th>num_defs</th>\n",
       "      <th>number_sentence_toks</th>\n",
       "      <th>number_def_toks</th>\n",
       "      <th>clipped_definition_toks</th>\n",
       "      <th>clipped_sentence_toks</th>\n",
       "      <th>clipped_word_tok_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>productivity</td>\n",
       "      <td>When|improvements|are|recommended|in|working|c...</td>\n",
       "      <td>30</td>\n",
       "      <td>productivity%1:07:00::</td>\n",
       "      <td>the quality of being productive or having the ...</td>\n",
       "      <td>the quality of being productive or having the ...</td>\n",
       "      <td>[the quality of being productive or having the...</td>\n",
       "      <td>When improvements are recommended in working ...</td>\n",
       "      <td>[3277, 18637, 527, 11349, 304, 3318, 4787, 482...</td>\n",
       "      <td>157</td>\n",
       "      <td>30</td>\n",
       "      <td>[ productivity: the quality of being productiv...</td>\n",
       "      <td>[[26206, 25, 279, 4367, 315, 1694, 27331, 477,...</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>[13, 23]</td>\n",
       "      <td>[[26206, 25, 279, 4367, 315, 1694, 27331, 477,...</td>\n",
       "      <td>[11349, 304, 3318, 4787, 482, 1778, 439, 18186...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>try</td>\n",
       "      <td>When|improvements|are|recommended|in|working|c...</td>\n",
       "      <td>21</td>\n",
       "      <td>try%2:41:00::</td>\n",
       "      <td>make an effort or attempt</td>\n",
       "      <td>earnest and conscientious activity intended to...</td>\n",
       "      <td>[earnest and conscientious activity intended t...</td>\n",
       "      <td>When improvements are recommended in working ...</td>\n",
       "      <td>[3277, 18637, 527, 11349, 304, 3318, 4787, 482...</td>\n",
       "      <td>124</td>\n",
       "      <td>21</td>\n",
       "      <td>[ try: earnest and conscientious activity inte...</td>\n",
       "      <td>[[1456, 25, 55349, 323, 74365, 1245, 5820, 108...</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>[13, 7, 17, 18, 6, 15, 7, 6, 16, 16]</td>\n",
       "      <td>[[1456, 25, 55349, 323, 74365, 1245, 5820, 108...</td>\n",
       "      <td>[11349, 304, 3318, 4787, 482, 1778, 439, 18186...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                           sentence  word_loc  \\\n",
       "85  productivity  When|improvements|are|recommended|in|working|c...        30   \n",
       "82           try  When|improvements|are|recommended|in|working|c...        21   \n",
       "\n",
       "                   wordnet                                         definition  \\\n",
       "85  productivity%1:07:00::  the quality of being productive or having the ...   \n",
       "82           try%2:41:00::                          make an effort or attempt   \n",
       "\n",
       "                                          definitions  \\\n",
       "85  the quality of being productive or having the ...   \n",
       "82  earnest and conscientious activity intended to...   \n",
       "\n",
       "                                             all_defs  \\\n",
       "85  [the quality of being productive or having the...   \n",
       "82  [earnest and conscientious activity intended t...   \n",
       "\n",
       "                                         sentence_str  \\\n",
       "85   When improvements are recommended in working ...   \n",
       "82   When improvements are recommended in working ...   \n",
       "\n",
       "                                        sentence_toks  word_start_index  \\\n",
       "85  [3277, 18637, 527, 11349, 304, 3318, 4787, 482...               157   \n",
       "82  [3277, 18637, 527, 11349, 304, 3318, 4787, 482...               124   \n",
       "\n",
       "    word_tok_index                                 definition_prompts  \\\n",
       "85              30  [ productivity: the quality of being productiv...   \n",
       "82              21  [ try: earnest and conscientious activity inte...   \n",
       "\n",
       "                                      definition_toks  num_defs  \\\n",
       "85  [[26206, 25, 279, 4367, 315, 1694, 27331, 477,...         2   \n",
       "82  [[1456, 25, 55349, 323, 74365, 1245, 5820, 108...        10   \n",
       "\n",
       "    number_sentence_toks                       number_def_toks  \\\n",
       "85                    35                              [13, 23]   \n",
       "82                    35  [13, 7, 17, 18, 6, 15, 7, 6, 16, 16]   \n",
       "\n",
       "                              clipped_definition_toks  \\\n",
       "85  [[26206, 25, 279, 4367, 315, 1694, 27331, 477,...   \n",
       "82  [[1456, 25, 55349, 323, 74365, 1245, 5820, 108...   \n",
       "\n",
       "                                clipped_sentence_toks  clipped_word_tok_index  \n",
       "85  [11349, 304, 3318, 4787, 482, 1778, 439, 18186...                      27  \n",
       "82  [11349, 304, 3318, 4787, 482, 1778, 439, 18186...                      18  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[df_new[\"clipped_word_tok_index\"] != df_new[\"word_tok_index\"]].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_clipped_token(row: pd.Series) -> pd.Series:\n",
    "    clipped_index = row[\"clipped_word_tok_index\"]\n",
    "    original_index = row[\"word_tok_index\"]\n",
    "\n",
    "    if 0 <= clipped_index < len(row[\"clipped_sentence_toks\"]):\n",
    "        clipped_token = row[\"clipped_sentence_toks\"][clipped_index]\n",
    "        original_token = row[\"sentence_toks\"][original_index]\n",
    "        row[\"clipped_token_matches\"] = (clipped_token == original_token)\n",
    "    else:\n",
    "        row[\"clipped_token_matches\"] = False\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.apply(validate_clipped_token, axis = 1)\n",
    "df_new[df_new[\"clipped_token_matches\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_definition_lengths(def_toks: List[List[int]], max_tok_len = 32) -> bool:\n",
    "    return all(len(lst) <= max_tok_len for lst in def_toks)\n",
    "\n",
    "\n",
    "def validate_clipped_sentence_length(row: pd.Series, max_tok_len = 32) -> pd.Series:\n",
    "    if \"clipped_sentence_toks\" not in row:\n",
    "        row[\"is_clipped_sentence_valid\"] = False\n",
    "    else:\n",
    "        row[\"is_clipped_sentence_valid\"] = (len(row[\"clipped_sentence_toks\"]) <= max_tok_len)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp[\"def_lens_valid\"] = df_tmp[\"clipped_definition_toks\"].apply(validate_definition_lengths)\n",
    "df_tmp = df_tmp.apply(validate_clipped_sentence_length, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number bad def rows:      0\n",
      "number bad sentence rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"number bad def rows:      {df_tmp[df_tmp[\"def_lens_valid\"] == False].shape[0]}\")\n",
    "print(f\"number bad sentence rows: {df_tmp[df_tmp[\"is_clipped_sentence_valid\"] == False].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the clipped sentences/definitions, with a clip level set for minimal loss of contextualising semantic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Currently the dataframe based approach is a bit awkward, it will be easier to extract all the lists of tokens, alongside lookup dicts based on the indices in the dataframe, then convert them to one big input tokens array, which we can chunk and send through the transformer in batches.\n",
    "\n",
    "The outputs can then be unpacked - it might be worth adding an output tensor dimension instead of the list approach currently used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
