{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnning Llama on the prompts\n",
    "\n",
    "Now we can test the full loop, before writing a job manifest, we will:\n",
    "1. load Llama into memory from our pickled weights\n",
    "2. normalised the data chunk to get prompts\n",
    "3. run the LLM on our promps\n",
    "4. save the hidden activations to disk\n",
    "\n",
    "5. (optional) implement caching for the definitions, since they may be shared between multiple words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7f9ee8760770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "tok = Tokenizer(tok_path)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-19 12:41:01,641:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Data/ModelWeights/llama_jax_weights.pkl\", \"rb\") as f:\n",
    "    params_jax_loaded = pickle.load(f)\n",
    "\n",
    "n_heads = 32\n",
    "n_kv_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Processed/SemCoreChunks/chunk_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def fix_whitespace(text):\n",
    "    # Remove spaces before punctuation\n",
    "    text = re.sub(r'\\s+([?.!,;:])', r'\\1', text)\n",
    "    # Ensure space after punctuation if followed by a word (except for some cases like commas within numbers)\n",
    "    text = re.sub(r'([?.!;:])(?=[^\\s])', r'\\1 ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "type series = pd.core.series.Series\n",
    "\n",
    "def prep_row(row: series) -> series:\n",
    "    row[\"all_defs\"] = str.split(row.definitions, '|')\n",
    "    row[\"sentence_str\"]  = ' ' + fix_whitespace(' '.join(str.split(row.sentence, '|')))  \n",
    "    row[\"sentence_toks\"] = tok.encode(row[\"sentence_str\"], bos = False, eos = False)\n",
    "\n",
    "    remaining = row[\"sentence_str\"]\n",
    "\n",
    "    word_start_index = 0\n",
    "\n",
    "    for i in range(row[\"word_loc\"]):\n",
    "        word = tok.decode([row[\"sentence_toks\"][i]])\n",
    "        word_start_index += remaining.find(word)\n",
    "        word_start_index += len(word)\n",
    "        remaining = row[\"sentence_str\"][word_start_index:]\n",
    "\n",
    "\n",
    "    row[\"word_start_index\"] = word_start_index\n",
    "\n",
    "    word_tok_index = 0\n",
    "    i = 0\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        word = tok.decode([row[\"sentence_toks\"][word_tok_index]])\n",
    "        \n",
    "        i += len(word)\n",
    "        \n",
    "        if i > word_start_index:\n",
    "            break\n",
    "        \n",
    "        word_tok_index  += 1\n",
    "\n",
    "    row[\"word_tok_index\"] = word_tok_index\n",
    "\n",
    "    encode = lambda  text : tok.encode(text, bos = False, eos = False)\n",
    "    if len(encode(\" \" + row[\"word\"])) == len(encode(\" \" + row[\"word\"] + \":\")):\n",
    "        return Exception(\"colon was absorbed - changing token embedding!\")\n",
    "    \n",
    "    defs = row[\"definitions\"].split('|')\n",
    "\n",
    "    row[\"definition_prompts\"] = [\" \" + row[\"word\"] + \": \" + d for d in defs]\n",
    "    row[\"definition_toks\"] = [encode(def_prompt) for def_prompt in row[\"definition_prompts\"]]\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 125.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_new = df.progress_apply(prep_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>How|long|has|it|been|since|you|reviewed|the|ob...</td>\n",
       "      <td>1</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>primarily temporal sense; being or indicating ...</td>\n",
       "      <td>desire strongly or persistently|primarily temp...</td>\n",
       "      <td>[desire strongly or persistently, primarily te...</td>\n",
       "      <td>How long has it been since you reviewed the o...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[ long: desire strongly or persistently,  long...</td>\n",
       "      <td>[[1317, 25, 12876, 16917, 477, 23135, 4501], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>been</td>\n",
       "      <td>How|long|has|it|been|since|you|reviewed|the|ob...</td>\n",
       "      <td>4</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "      <td>have the quality of being; (copula, used with ...</td>\n",
       "      <td>a light strong brittle grey toxic bivalent met...</td>\n",
       "      <td>[a light strong brittle grey toxic bivalent me...</td>\n",
       "      <td>How long has it been since you reviewed the o...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>[ been: a light strong brittle grey toxic biva...</td>\n",
       "      <td>[[1027, 25, 264, 3177, 3831, 95749, 20366, 215...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word                                           sentence  word_loc  \\\n",
       "0  long  How|long|has|it|been|since|you|reviewed|the|ob...         1   \n",
       "1  been  How|long|has|it|been|since|you|reviewed|the|ob...         4   \n",
       "\n",
       "          wordnet                                         definition  \\\n",
       "0  long%3:00:02::  primarily temporal sense; being or indicating ...   \n",
       "1    be%2:42:03::  have the quality of being; (copula, used with ...   \n",
       "\n",
       "                                         definitions  \\\n",
       "0  desire strongly or persistently|primarily temp...   \n",
       "1  a light strong brittle grey toxic bivalent met...   \n",
       "\n",
       "                                            all_defs  \\\n",
       "0  [desire strongly or persistently, primarily te...   \n",
       "1  [a light strong brittle grey toxic bivalent me...   \n",
       "\n",
       "                                        sentence_str  \\\n",
       "0   How long has it been since you reviewed the o...   \n",
       "1   How long has it been since you reviewed the o...   \n",
       "\n",
       "                                       sentence_toks  word_start_index  \\\n",
       "0  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                 4   \n",
       "1  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                16   \n",
       "\n",
       "   word_tok_index                                 definition_prompts  \\\n",
       "0               1  [ long: desire strongly or persistently,  long...   \n",
       "1               4  [ been: a light strong brittle grey toxic biva...   \n",
       "\n",
       "                                     definition_toks  \n",
       "0  [[1317, 25, 12876, 16917, 477, 23135, 4501], [...  \n",
       "1  [[1027, 25, 264, 3177, 3831, 95749, 20366, 215...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_jax_loaded[\"freqs_cis\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from LLM.llama_jax.model import reporting_transformer\n",
    "\n",
    "\n",
    "jitted_transformer = jax.jit(reporting_transformer, static_argnames=[\"n_heads\", \"n_kv_heads\"])\n",
    "\n",
    "\n",
    "max_tok_len = 256\n",
    "\n",
    "params_jax_loaded[\"freqs_cis\"] = params_jax_loaded[\"freqs_cis\"][0:max_tok_len, :]\n",
    "\n",
    "def pad_toks(toks: List[int], max_len = max_tok_len):\n",
    "    to_pad = max_len - len(toks)\n",
    "\n",
    "    if (to_pad < 0):\n",
    "        ret = toks[0:to_pad]\n",
    "    else:\n",
    "        ret = toks + [0] * to_pad\n",
    "\n",
    "    mask = [0 if tok != 0 else -jnp.inf for tok in ret]\n",
    "\n",
    "    return jnp.array(ret)[None, :], jnp.array(mask)[None, :]\n",
    "\n",
    "\n",
    "def get_activations(toks: List[int]):\n",
    "    padded_toks, mask = pad_toks(toks)\n",
    "    return jitted_transformer(padded_toks, params_jax_loaded, mask, n_heads, n_kv_heads)\n",
    "\n",
    "\n",
    "def LLM_process_row(row: series) -> series:\n",
    "    sentence_acts = get_activations(row.sentence_toks)\n",
    "    def_acts     = [get_activations(def_toks) for def_toks in row.definition_toks]\n",
    "\n",
    "    # TODO - if the data volume is too large, then only store the activations\n",
    "    # near the word itself\n",
    "    row[\"sentence_activations\"] = sentence_acts\n",
    "    row[\"definition_activates\"] = def_acts\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_small = df_new.head(1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "#df_new_small = df_new_small.progress_apply(LLM_process_row, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new_small.iloc[0].sentence_activations[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data volume\n",
    "\n",
    "We are seeing roughly 1Mb of data per layer, so 16Mb per sentence/definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"num_defs\"] = df_new[\"definition_toks\"].apply(lambda ds : len(ds))\n",
    "df_new[\"num_defs\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have 1000 setences + definitions an so will be generating 16Gb of data per chunk, and thus would need ~30Tb of storage to process it all!\n",
    "\n",
    "Lets cut down the volume, first lets see what is actually a reasonable max token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"number_sentence_toks\"] = df_new[\"sentence_toks\"].apply(lambda toks : len(toks))\n",
    "df_new[\"number_def_toks\"] = df_new[\"definition_toks\"].apply(lambda defs_toks : [len(def_toks) for def_toks in defs_toks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean      20.710000\n",
       "std        8.210974\n",
       "min        6.000000\n",
       "25%       13.000000\n",
       "50%       19.000000\n",
       "75%       27.000000\n",
       "max       35.000000\n",
       "Name: number_sentence_toks, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"number_sentence_toks\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Third': 13.5, 'Median': 9.0, 'Mean': 10.783902976846747, 'Max': 42}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_values = [num for sublist in df_new['number_def_toks'] for num in sublist]\n",
    "\n",
    "# Compute statistics\n",
    "third = np.percentile(all_values, 75)\n",
    "median = np.median(all_values)\n",
    "mean = np.mean(all_values)\n",
    "max_val = np.max(all_values)\n",
    "\n",
    "# Print results\n",
    "stats = {\n",
    "    \"Third\" : third,\n",
    "    \"Median\": median,\n",
    "    \"Mean\": mean,\n",
    "    \"Max\": max_val,\n",
    "}\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limiting the inputs to 32 tokens looks like it would still keep most of the information we need, the definitions can be clipped from the ends... for the sentences we should clip outwards from the word, luckily we have the word token indices for that\n",
    "\n",
    "That will yield a 3-fold (8x) reduction in data volume to roughly 4Tb. However we will not necessarily process all the data, and 500Mb per chunk seems more manageable for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_definition_token_list(def_toks: List[List[int]], max_tok_len = 32) -> List[List[int]]:\n",
    "    ret = []\n",
    "    for _, lst in enumerate(def_toks):\n",
    "        if len(lst) <= max_tok_len:\n",
    "            ret.append(lst)\n",
    "        else:\n",
    "            ret.append(lst[0:max_tok_len])\n",
    "    return ret\n",
    "\n",
    "def clip_sentence_token_list(row: series, max_tok_len = 32) -> series:\n",
    "\n",
    "    range = (0,0)\n",
    "    new_index = row.word_tok_index\n",
    "\n",
    "    if len(row.sentence_toks) <= max_tok_len:\n",
    "        range = (0,None)\n",
    "    elif (row.word_tok_index <= max_tok_len // 2):\n",
    "        range = (0,max_tok_len)\n",
    "    elif (len(row.sentence_toks) - row.word_tok_index <= max_tok_len // 2):\n",
    "        range = (-max_tok_len, None)\n",
    "        new_index -= (len(row.sentence_toks) - max_tok_len) # this much has been removed\n",
    "    else:\n",
    "        delta = max_tok_len // 2 \n",
    "        mod   = max_tok_len %  2\n",
    "        mid = row.word_tok_index\n",
    "        range = (mid - delta, mid + delta + mod)\n",
    "        new_index -= (mid - delta)\n",
    "\n",
    "    row[\"clipped_sentence_toks\"] = row.sentence_toks[range[0]:range[1]]\n",
    "    row[\"clipped_word_tok_index\"] = new_index\n",
    "\n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# tests a la chatgpt\n",
    "\n",
    "# 1) Sentence length <= max_tok_len\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": [\"I\", \"love\", \"Python\"],\n",
    "    \"word_tok_index\": 1\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=5)\n",
    "assert row[\"clipped_sentence_toks\"] == [\"I\", \"love\", \"Python\"]\n",
    "assert row[\"clipped_word_tok_index\"] == 1\n",
    "\n",
    "# 2) Focus token near the beginning\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": [\"Token\"] * 20,\n",
    "    \"word_tok_index\": 2\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=6)\n",
    "# Should keep from index 0 up to index 6, word_tok_index should remain 2\n",
    "assert len(row[\"clipped_sentence_toks\"]) == 6\n",
    "assert row[\"clipped_word_tok_index\"] == 2\n",
    "\n",
    "# 3) Focus token near the end\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": [\"Token\"] * 20,\n",
    "    \"word_tok_index\": 18\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=5)\n",
    "# Should keep last 5 tokens\n",
    "# Original indices: 15,16,17,18,19\n",
    "# 'word_tok_index' = 18 => new_index = 18 - 15 = 3\n",
    "assert len(row[\"clipped_sentence_toks\"]) == 5\n",
    "assert row[\"clipped_word_tok_index\"] == 3\n",
    "\n",
    "# 4) Focus token somewhere in the middle\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": list(range(30)),  # Just use integer tokens for clarity\n",
    "    \"word_tok_index\": 15\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=6)\n",
    "# max_tok_len=6 => delta=3 => mod=0\n",
    "# start=15-3=12, end=15+3=18 => slice is [12,13,14,15,16,17]\n",
    "# new_index = 15 - 12 = 3\n",
    "assert row[\"clipped_sentence_toks\"] == [12, 13, 14, 15, 16, 17]\n",
    "assert row[\"clipped_word_tok_index\"] == 3\n",
    "\n",
    "# 5) Edge case: focus token exactly on boundary (like index=10, max_tok_len=6)\n",
    "row = pd.Series({\n",
    "    \"sentence_toks\": list(range(20)),\n",
    "    \"word_tok_index\": 10\n",
    "})\n",
    "row = clip_sentence_token_list(row, max_tok_len=6)\n",
    "# delta=3, mod=0 => slice [7..13) => [7,8,9,10,11,12], length=6\n",
    "# new_index = 10 - 7 = 3\n",
    "assert len(row[\"clipped_sentence_toks\"]) == 6\n",
    "assert row[\"clipped_word_tok_index\"] == 3\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"clipped_definition_toks\"] = df_new[\"definition_toks\"].apply(clip_definition_token_list)\n",
    "df_new = df_new.apply(clip_sentence_token_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "      <th>num_defs</th>\n",
       "      <th>number_sentence_toks</th>\n",
       "      <th>number_def_toks</th>\n",
       "      <th>clipped_definition_toks</th>\n",
       "      <th>clipped_sentence_toks</th>\n",
       "      <th>clipped_word_tok_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>productivity</td>\n",
       "      <td>When|improvements|are|recommended|in|working|c...</td>\n",
       "      <td>30</td>\n",
       "      <td>productivity%1:07:00::</td>\n",
       "      <td>the quality of being productive or having the ...</td>\n",
       "      <td>the quality of being productive or having the ...</td>\n",
       "      <td>[the quality of being productive or having the...</td>\n",
       "      <td>When improvements are recommended in working ...</td>\n",
       "      <td>[3277, 18637, 527, 11349, 304, 3318, 4787, 482...</td>\n",
       "      <td>157</td>\n",
       "      <td>30</td>\n",
       "      <td>[ productivity: the quality of being productiv...</td>\n",
       "      <td>[[26206, 25, 279, 4367, 315, 1694, 27331, 477,...</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>[13, 23]</td>\n",
       "      <td>[[26206, 25, 279, 4367, 315, 1694, 27331, 477,...</td>\n",
       "      <td>[11349, 304, 3318, 4787, 482, 1778, 439, 18186...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>try</td>\n",
       "      <td>When|improvements|are|recommended|in|working|c...</td>\n",
       "      <td>21</td>\n",
       "      <td>try%2:41:00::</td>\n",
       "      <td>make an effort or attempt</td>\n",
       "      <td>earnest and conscientious activity intended to...</td>\n",
       "      <td>[earnest and conscientious activity intended t...</td>\n",
       "      <td>When improvements are recommended in working ...</td>\n",
       "      <td>[3277, 18637, 527, 11349, 304, 3318, 4787, 482...</td>\n",
       "      <td>124</td>\n",
       "      <td>21</td>\n",
       "      <td>[ try: earnest and conscientious activity inte...</td>\n",
       "      <td>[[1456, 25, 55349, 323, 74365, 1245, 5820, 108...</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>[13, 7, 17, 18, 6, 15, 7, 6, 16, 16]</td>\n",
       "      <td>[[1456, 25, 55349, 323, 74365, 1245, 5820, 108...</td>\n",
       "      <td>[11349, 304, 3318, 4787, 482, 1778, 439, 18186...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                           sentence  word_loc  \\\n",
       "85  productivity  When|improvements|are|recommended|in|working|c...        30   \n",
       "82           try  When|improvements|are|recommended|in|working|c...        21   \n",
       "\n",
       "                   wordnet                                         definition  \\\n",
       "85  productivity%1:07:00::  the quality of being productive or having the ...   \n",
       "82           try%2:41:00::                          make an effort or attempt   \n",
       "\n",
       "                                          definitions  \\\n",
       "85  the quality of being productive or having the ...   \n",
       "82  earnest and conscientious activity intended to...   \n",
       "\n",
       "                                             all_defs  \\\n",
       "85  [the quality of being productive or having the...   \n",
       "82  [earnest and conscientious activity intended t...   \n",
       "\n",
       "                                         sentence_str  \\\n",
       "85   When improvements are recommended in working ...   \n",
       "82   When improvements are recommended in working ...   \n",
       "\n",
       "                                        sentence_toks  word_start_index  \\\n",
       "85  [3277, 18637, 527, 11349, 304, 3318, 4787, 482...               157   \n",
       "82  [3277, 18637, 527, 11349, 304, 3318, 4787, 482...               124   \n",
       "\n",
       "    word_tok_index                                 definition_prompts  \\\n",
       "85              30  [ productivity: the quality of being productiv...   \n",
       "82              21  [ try: earnest and conscientious activity inte...   \n",
       "\n",
       "                                      definition_toks  num_defs  \\\n",
       "85  [[26206, 25, 279, 4367, 315, 1694, 27331, 477,...         2   \n",
       "82  [[1456, 25, 55349, 323, 74365, 1245, 5820, 108...        10   \n",
       "\n",
       "    number_sentence_toks                       number_def_toks  \\\n",
       "85                    35                              [13, 23]   \n",
       "82                    35  [13, 7, 17, 18, 6, 15, 7, 6, 16, 16]   \n",
       "\n",
       "                              clipped_definition_toks  \\\n",
       "85  [[26206, 25, 279, 4367, 315, 1694, 27331, 477,...   \n",
       "82  [[1456, 25, 55349, 323, 74365, 1245, 5820, 108...   \n",
       "\n",
       "                                clipped_sentence_toks  clipped_word_tok_index  \n",
       "85  [11349, 304, 3318, 4787, 482, 1778, 439, 18186...                      27  \n",
       "82  [11349, 304, 3318, 4787, 482, 1778, 439, 18186...                      18  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[df_new[\"clipped_word_tok_index\"] != df_new[\"word_tok_index\"]].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_clipped_token(row: pd.Series) -> pd.Series:\n",
    "    clipped_index = row[\"clipped_word_tok_index\"]\n",
    "    original_index = row[\"word_tok_index\"]\n",
    "\n",
    "    if 0 <= clipped_index < len(row[\"clipped_sentence_toks\"]):\n",
    "        clipped_token = row[\"clipped_sentence_toks\"][clipped_index]\n",
    "        original_token = row[\"sentence_toks\"][original_index]\n",
    "        row[\"clipped_token_matches\"] = (clipped_token == original_token)\n",
    "    else:\n",
    "        row[\"clipped_token_matches\"] = False\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "      <th>num_defs</th>\n",
       "      <th>number_sentence_toks</th>\n",
       "      <th>number_def_toks</th>\n",
       "      <th>clipped_definition_toks</th>\n",
       "      <th>clipped_sentence_toks</th>\n",
       "      <th>clipped_word_tok_index</th>\n",
       "      <th>clipped_token_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word, sentence, word_loc, wordnet, definition, definitions, all_defs, sentence_str, sentence_toks, word_start_index, word_tok_index, definition_prompts, definition_toks, num_defs, number_sentence_toks, number_def_toks, clipped_definition_toks, clipped_sentence_toks, clipped_word_tok_index, clipped_token_matches]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df_new.apply(validate_clipped_token, axis = 1)\n",
    "df_new[df_new[\"clipped_token_matches\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_definition_lengths(def_toks: List[List[int]], max_tok_len = 32) -> bool:\n",
    "    return all(len(lst) <= max_tok_len for lst in def_toks)\n",
    "\n",
    "\n",
    "def validate_clipped_sentence_length(row: pd.Series, max_tok_len = 32) -> pd.Series:\n",
    "    if \"clipped_sentence_toks\" not in row:\n",
    "        row[\"is_clipped_sentence_valid\"] = False\n",
    "    else:\n",
    "        row[\"is_clipped_sentence_valid\"] = (len(row[\"clipped_sentence_toks\"]) <= max_tok_len)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp[\"def_lens_valid\"] = df_tmp[\"clipped_definition_toks\"].apply(validate_definition_lengths)\n",
    "df_tmp = df_tmp.apply(validate_clipped_sentence_length, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number bad def rows:      0\n",
      "number bad sentence rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"number bad def rows:      {df_tmp[df_tmp[\"def_lens_valid\"] == False].shape[0]}\")\n",
    "print(f\"number bad sentence rows: {df_tmp[df_tmp[\"is_clipped_sentence_valid\"] == False].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the clipped sentences/definitions, with a clip level set for minimal loss of contextualising semantic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further data reduction volume\n",
    "\n",
    "Do we need to save all the tokens' hidden activations? probably not - only the ones near the word of interest are likely to be helpful.\n",
    "\n",
    "For the definition, these will be the [0,n] tokens\n",
    "\n",
    "For the sentence, these will be the [w - n, w + n] tokens, since the word may appear anywhere, we should pad this if the word is near the start/end of the sentence.\n",
    "\n",
    "A sensible choice of n would be e.g. 5, so 10 activations per sentence, and 5 per definitions, roughly a 4x data reduction, so only 1Tb of data, or 125Mb per chunk.\n",
    "\n",
    "We still choose to reduce the sentence length as well, since that means we can batch more sentences/definitions per pass of the transformer,\n",
    "\n",
    "This data reduction is probably best saved for post-processing before saving, since it will require access to the word token indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Currently the dataframe based approach is a bit awkward, it will be easier to extract all the lists of tokens, alongside lookup dicts based on the indices in the dataframe, then convert them to one big input tokens array, which we can chunk and send through the transformer in batches.\n",
    "\n",
    "This will also be a good location to inject the cacheing logic, and short circuit the call to the LLM.\n",
    "\n",
    "The outputs can then be unpacked - it might be worth adding an output tensor dimension instead of the list approach currently used.\n",
    "\n",
    "Finally we should have a method to extract the indices of the activations that we actually care about.\n",
    "\n",
    "### Costly operations\n",
    "\n",
    "Previously we saw that calling the LLM was very costly in terms of time, however now with the 32 token max sentence limit - the bottleneck is now working with the large arrays of activations that are output.\n",
    "\n",
    "We should keep indexing information for how to select the regions of these activations we are interested in, and keep that using Jax, to minimise the data volume that ends up being parsed into a pandas array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_loc</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>definition</th>\n",
       "      <th>definitions</th>\n",
       "      <th>all_defs</th>\n",
       "      <th>sentence_str</th>\n",
       "      <th>sentence_toks</th>\n",
       "      <th>word_start_index</th>\n",
       "      <th>word_tok_index</th>\n",
       "      <th>definition_prompts</th>\n",
       "      <th>definition_toks</th>\n",
       "      <th>num_defs</th>\n",
       "      <th>number_sentence_toks</th>\n",
       "      <th>number_def_toks</th>\n",
       "      <th>clipped_definition_toks</th>\n",
       "      <th>clipped_sentence_toks</th>\n",
       "      <th>clipped_word_tok_index</th>\n",
       "      <th>clipped_token_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>How|long|has|it|been|since|you|reviewed|the|ob...</td>\n",
       "      <td>1</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>primarily temporal sense; being or indicating ...</td>\n",
       "      <td>desire strongly or persistently|primarily temp...</td>\n",
       "      <td>[desire strongly or persistently, primarily te...</td>\n",
       "      <td>How long has it been since you reviewed the o...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[ long: desire strongly or persistently,  long...</td>\n",
       "      <td>[[1317, 25, 12876, 16917, 477, 23135, 4501], [...</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>[7, 26, 19, 11, 5, 13, 14, 5, 9, 11, 11, 6]</td>\n",
       "      <td>[[1317, 25, 12876, 16917, 477, 23135, 4501], [...</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word                                           sentence  word_loc  \\\n",
       "0  long  How|long|has|it|been|since|you|reviewed|the|ob...         1   \n",
       "\n",
       "          wordnet                                         definition  \\\n",
       "0  long%3:00:02::  primarily temporal sense; being or indicating ...   \n",
       "\n",
       "                                         definitions  \\\n",
       "0  desire strongly or persistently|primarily temp...   \n",
       "\n",
       "                                            all_defs  \\\n",
       "0  [desire strongly or persistently, primarily te...   \n",
       "\n",
       "                                        sentence_str  \\\n",
       "0   How long has it been since you reviewed the o...   \n",
       "\n",
       "                                       sentence_toks  word_start_index  \\\n",
       "0  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                 4   \n",
       "\n",
       "   word_tok_index                                 definition_prompts  \\\n",
       "0               1  [ long: desire strongly or persistently,  long...   \n",
       "\n",
       "                                     definition_toks  num_defs  \\\n",
       "0  [[1317, 25, 12876, 16917, 477, 23135, 4501], [...        12   \n",
       "\n",
       "   number_sentence_toks                              number_def_toks  \\\n",
       "0                    17  [7, 26, 19, 11, 5, 13, 14, 5, 9, 11, 11, 6]   \n",
       "\n",
       "                             clipped_definition_toks  \\\n",
       "0  [[1317, 25, 12876, 16917, 477, 23135, 4501], [...   \n",
       "\n",
       "                               clipped_sentence_toks  clipped_word_tok_index  \\\n",
       "0  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...                       1   \n",
       "\n",
       "   clipped_token_matches  \n",
       "0                   True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_tokens(tokens: List[int], index: int):\n",
    "    token_str = ','.join(map(str, tokens)) + str(index)\n",
    "    return hashlib.sha256(token_str.encode()).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "def transform_dataframe(df, activation_width = 4):\n",
    "    records = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "\n",
    "        # process sentence toks\n",
    "        sent_toks = row['clipped_sentence_toks']\n",
    "        tok_index = row['clipped_word_tok_index']\n",
    "        records.append((idx, sent_toks, 'sentence', -1, hash_tokens(sent_toks, tok_index), tok_index, tok_index-activation_width))\n",
    "        \n",
    "        # process definitions' toks\n",
    "        for def_idx, def_toks in enumerate(row['clipped_definition_toks']):\n",
    "            records.append((idx, def_toks, 'definition', def_idx, hash_tokens(def_toks, 0), 0, 0-activation_width)) # definitions are always the first token\n",
    "    new_df = pd.DataFrame(records, columns=['map_index', 'toks', 'column', 'def_index', 'hash', 'clipped_word_tok_index', 'istart'])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1452.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_index</th>\n",
       "      <th>toks</th>\n",
       "      <th>column</th>\n",
       "      <th>def_index</th>\n",
       "      <th>hash</th>\n",
       "      <th>clipped_word_tok_index</th>\n",
       "      <th>istart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[2650, 1317, 706, 433, 1027, 2533, 499, 22690,...</td>\n",
       "      <td>sentence</td>\n",
       "      <td>-1</td>\n",
       "      <td>e5628f2b996830cd285a609e72bf7fbce8666e0b4f7d49...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[1317, 25, 12876, 16917, 477, 23135, 4501]</td>\n",
       "      <td>definition</td>\n",
       "      <td>0</td>\n",
       "      <td>e04ce50c948bb2e03144f8b8ada93591c11c89d6c7d355...</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[1317, 25, 15871, 37015, 5647, 26, 1694, 477, ...</td>\n",
       "      <td>definition</td>\n",
       "      <td>1</td>\n",
       "      <td>5e724cd29a7137fb48b65d0719ffe65f5032fdde33f6a3...</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[1317, 25, 15871, 29079, 5647, 26, 315, 12309,...</td>\n",
       "      <td>definition</td>\n",
       "      <td>2</td>\n",
       "      <td>e71bf88afe0c699b6990b2170503455216ad12ec401880...</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[1317, 25, 315, 12309, 2294, 2673, 26, 482, 17...</td>\n",
       "      <td>definition</td>\n",
       "      <td>3</td>\n",
       "      <td>bbcf55681897f86d0d249055d0a34d53e2957bb154050b...</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   map_index                                               toks      column  \\\n",
       "0          0  [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...    sentence   \n",
       "1          0         [1317, 25, 12876, 16917, 477, 23135, 4501]  definition   \n",
       "2          0  [1317, 25, 15871, 37015, 5647, 26, 1694, 477, ...  definition   \n",
       "3          0  [1317, 25, 15871, 29079, 5647, 26, 315, 12309,...  definition   \n",
       "4          0  [1317, 25, 315, 12309, 2294, 2673, 26, 482, 17...  definition   \n",
       "\n",
       "   def_index                                               hash  \\\n",
       "0         -1  e5628f2b996830cd285a609e72bf7fbce8666e0b4f7d49...   \n",
       "1          0  e04ce50c948bb2e03144f8b8ada93591c11c89d6c7d355...   \n",
       "2          1  5e724cd29a7137fb48b65d0719ffe65f5032fdde33f6a3...   \n",
       "3          2  e71bf88afe0c699b6990b2170503455216ad12ec401880...   \n",
       "4          3  bbcf55681897f86d0d249055d0a34d53e2957bb154050b...   \n",
       "\n",
       "   clipped_word_tok_index  istart  \n",
       "0                       1      -3  \n",
       "1                       0      -4  \n",
       "2                       0      -4  \n",
       "3                       0      -4  \n",
       "4                       0      -4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_df = transform_dataframe(df_new, 4)\n",
    "toks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old shape (100, 20)\n",
      "new shape (1007, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"old shape {df_new.shape}\")\n",
    "print(f\"new shape {toks_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a 10-fold increase in data volume after putting each list of tokens in its own dataframe row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use negative/off the end indexing here, because after we have the activations we will zero pad +-width to deal with words near the start/end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length was 1007\n",
      "unique token lists: 830\n"
     ]
    }
   ],
   "source": [
    "print(f\"original length was {toks_df.shape[0]}\")\n",
    "\n",
    "unique_toks_df = toks_df.drop_duplicates(subset=['hash']).copy().reset_index(drop = True)\n",
    "\n",
    "print(f\"unique token lists: {unique_toks_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_toks_df[\"len\"] = unique_toks_df['toks'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/830 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 830/830 [00:04<00:00, 178.52it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_toks_df[[\"padded_toks\", \"mask\"]] = unique_toks_df[\"toks\"].progress_apply(lambda lst: pad_toks(lst, 32)).apply(pd.Series)\n",
    "# padding at the end so our indices are still valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_toks_df[\"tok_shape\"] = unique_toks_df['padded_toks'].apply(lambda arr : arr.shape)\n",
    "unique_toks_df[\"mask_shape\"] = unique_toks_df['mask'].apply(lambda arr : arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "unique_toks_df['batch_id'] = unique_toks_df.index // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the tokens ready to be passed through the LLM, we can stack them, run the transformer, then unpack the results back into the rows, lets develop that function now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = unique_toks_df[unique_toks_df['batch_id'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.concatenate(batch_df['padded_toks'], axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_toks.shape=(64, 32)\n",
      "batch_masks.shape=(64, 32)\n"
     ]
    }
   ],
   "source": [
    "batch_toks = jnp.concatenate(batch_df['padded_toks'], axis = 0)\n",
    "batch_masks = jnp.concatenate(batch_df['mask'], axis = 0)\n",
    "\n",
    "print(f\"{batch_toks.shape=}\")\n",
    "print(f\"{batch_masks.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_jax_loaded[\"freqs_cis\"] = params_jax_loaded[\"freqs_cis\"][0:32, :] # now need even fewer freq cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_activations = jitted_transformer(batch_toks, params_jax_loaded, batch_masks, n_heads, n_kv_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 32, 2048)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -3\n",
       "1    -4\n",
       "2    -4\n",
       "3    -4\n",
       "4    -4\n",
       "5    -4\n",
       "6    -4\n",
       "7    -4\n",
       "8    -4\n",
       "9    -4\n",
       "10   -4\n",
       "11   -4\n",
       "12   -4\n",
       "13    0\n",
       "14   -4\n",
       "15   -4\n",
       "Name: istart, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df['istart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _slice_activations_body(activations: jax.Array, indices: jax.Array, width: int):\n",
    "\n",
    "\n",
    "    batch, layer, toks, hidden = activations.shape\n",
    "\n",
    "    # pad activations to handle edge cases\n",
    "    padded_activations = jnp.pad(activations, ((0, 0), (0, 0), (width, width), (0, 0)), mode='constant')\n",
    "\n",
    "    # adjust indices to account for padding\n",
    "    adjusted_indices = jnp.ravel(indices + width)  # Shape: (batch,)\n",
    "    \n",
    "    # Define function for dynamic slicing\n",
    "    def extract_slice(activs, start):\n",
    "        slice_shape = (layer, 9, hidden)  # Static slice shape (layer, 9, hidden)\n",
    "        return jax.lax.dynamic_slice(activs, (0, start, 0), slice_shape)\n",
    "\n",
    "    # vectorize over batch dimension\n",
    "    vmap_extract = jax.vmap(extract_slice, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "    result = vmap_extract(padded_activations, adjusted_indices)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "_slice_activations_updated = jax.jit(_slice_activations_body, static_argnames=['width'])\n",
    "\n",
    "def slice_activations(activations: jax.Array, indices: pd.DataFrame, width: int = 4):\n",
    "    jax_indices = jnp.array(indices.to_numpy())  \n",
    "    return _slice_activations_updated(activations, jax_indices, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 9, 2048)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_activations = slice_activations(batch_activations, batch_df[['istart']], width=4)\n",
    "sliced_activations.shape  # Expected output: (16, 16, 9, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 32, 2048)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.0119629, -0.00976562, -0.0106201, ..., -0.0142822, -0.0361328,\n",
       "       -6.10352e-05], dtype=bfloat16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_activations[0,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.0119629, -0.00976562, -0.0106201, ..., -0.0142822, -0.0361328,\n",
       "       -6.10352e-05], dtype=bfloat16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_activations[0,0,3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see this lines up with the reindexing we anticipated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_outputs = [batch_activations[i] for i in tqdm(range(batch_size))]\n",
    "\n",
    "batch_df[\"model_output\"] = list(sliced_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map_index                                                                 0\n",
       "toks                      [2650, 1317, 706, 433, 1027, 2533, 499, 22690,...\n",
       "column                                                             sentence\n",
       "def_index                                                                -1\n",
       "hash                      e5628f2b996830cd285a609e72bf7fbce8666e0b4f7d49...\n",
       "clipped_word_tok_index                                                    1\n",
       "istart                                                                   -3\n",
       "len                                                                      17\n",
       "padded_toks               [[2650, 1317, 706, 433, 1027, 2533, 499, 22690...\n",
       "mask                      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "tok_shape                                                           (1, 32)\n",
       "mask_shape                                                          (1, 32)\n",
       "batch_id                                                                  0\n",
       "model_output              [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.0319824, 0.050293, -0.0712891, ..., -0.0116577, -0.0292969,\n",
       "         0.0055542],\n",
       "        [-0.0512695, -0.0184326, -0.0620117, ..., 0.00732422,\n",
       "         -0.0088501, 0.000671387],\n",
       "        [0.0390625, 0.0339355, -0.0805664, ..., 0.0561523, -0.0800781,\n",
       "         0.0057373]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.0400391, 0.0600586, -0.124023, ..., 0.0108643, -0.0203857,\n",
       "         0.0664062],\n",
       "        [-0.0600586, -0.0187988, -0.0493164, ..., 0.048584, -0.0264893,\n",
       "         -0.0114746],\n",
       "        [0.081543, 0.0688477, -0.0986328, ..., 0.0205078, -0.128906,\n",
       "         0.0397949]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [-0.0402832, 0.103516, -0.125977, ..., -0.0463867, -0.120117,\n",
       "         0.0263672],\n",
       "        [-0.0395508, 0.0395508, -0.0510254, ..., -0.0603027, -0.134766,\n",
       "         0.0371094],\n",
       "        [0.0229492, 0.0527344, -0.0510254, ..., 0.0253906, -0.308594,\n",
       "         0.0922852]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [-0.222656, -0.46875, -0.613281, ..., 0.0195312, -0.0415039,\n",
       "         0.171875],\n",
       "        [-0.261719, -0.398438, -0.0957031, ..., -0.0537109, -0.155273,\n",
       "         0.182617],\n",
       "        [-0.232422, -0.265625, -0.188477, ..., 0.09375, -0.241211,\n",
       "         0.232422]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.355469, -0.0291748, -0.414062, ..., -0.21875, -0.308594,\n",
       "         -0.110352],\n",
       "        [0.371094, 0.0996094, 0.212891, ..., -0.394531, -0.429688,\n",
       "         -0.0595703],\n",
       "        [0.316406, 0.291016, 0.0751953, ..., -0.287109, -0.445312,\n",
       "         -0.09375]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.300781, 0.960938, -0.078125, ..., -0.691406, -1.42188,\n",
       "         0.081543],\n",
       "        [0.328125, 1.09375, 0.273438, ..., -1.00781, -1.49219, 0.148438],\n",
       "        [0.202148, 1.32031, 0.585938, ..., -1.03125, -1.55469, 0.121094]]],      dtype=bfloat16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.iloc[0][\"model_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9, 2048)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df['model_output'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress\n",
    "\n",
    "We now have the activations [word-4,word+4] with the word centred, all that remains is to do this for all the batches, map the data back to the original dataframe and write it to disk.\n",
    "\n",
    "We use a filestore for the jax arrays as they are better saved in a terse format (pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dfs = [group.reset_index(drop=True) for _, group in unique_toks_df.groupby('batch_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_index</th>\n",
       "      <th>toks</th>\n",
       "      <th>column</th>\n",
       "      <th>def_index</th>\n",
       "      <th>hash</th>\n",
       "      <th>clipped_word_tok_index</th>\n",
       "      <th>istart</th>\n",
       "      <th>len</th>\n",
       "      <th>padded_toks</th>\n",
       "      <th>mask</th>\n",
       "      <th>tok_shape</th>\n",
       "      <th>mask_shape</th>\n",
       "      <th>batch_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[2532, 25, 12152, 7061, 320, 16381, 304, 9635,...</td>\n",
       "      <td>definition</td>\n",
       "      <td>6</td>\n",
       "      <td>e1f23e6cd017b0bd2624612740dd42d83ce37623012542...</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>25</td>\n",
       "      <td>[[2532, 25, 12152, 7061, 320, 16381, 304, 9635...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>(1, 32)</td>\n",
       "      <td>(1, 32)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   map_index                                               toks      column  \\\n",
       "0          5  [2532, 25, 12152, 7061, 320, 16381, 304, 9635,...  definition   \n",
       "\n",
       "   def_index                                               hash  \\\n",
       "0          6  e1f23e6cd017b0bd2624612740dd42d83ce37623012542...   \n",
       "\n",
       "   clipped_word_tok_index  istart  len  \\\n",
       "0                       0      -4   25   \n",
       "\n",
       "                                         padded_toks  \\\n",
       "0  [[2532, 25, 12152, 7061, 320, 16381, 304, 9635...   \n",
       "\n",
       "                                                mask tok_shape mask_shape  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   (1, 32)    (1, 32)   \n",
       "\n",
       "   batch_id  \n",
       "0         1  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dfs[1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    batch_toks = jnp.concatenate(batch_df['padded_toks'], axis = 0)\n",
    "    batch_masks = jnp.concatenate(batch_df['mask'], axis = 0)\n",
    "\n",
    "    batch_activations = jitted_transformer(batch_toks, params_jax_loaded, batch_masks, n_heads, n_kv_heads)\n",
    "    sliced_activations = slice_activations(batch_activations, batch_df[['istart']], width=4)\n",
    "\n",
    "    batch_df[\"model_output\"] = list(sliced_activations)\n",
    "\n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 13/13 [13:52<00:00, 64.05s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_batch_dfs = [process_batch(batch_df) for batch_df in tqdm(batch_dfs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.0319824, 0.050293, -0.0712891, ..., -0.0116577, -0.0292969,\n",
       "         0.0055542],\n",
       "        [-0.0512695, -0.0184326, -0.0620117, ..., 0.00732422,\n",
       "         -0.0088501, 0.000671387],\n",
       "        [0.0390625, 0.0339355, -0.0805664, ..., 0.0561523, -0.0800781,\n",
       "         0.0057373]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.0400391, 0.0600586, -0.124023, ..., 0.0108643, -0.0203857,\n",
       "         0.0664062],\n",
       "        [-0.0600586, -0.0187988, -0.0493164, ..., 0.048584, -0.0264893,\n",
       "         -0.0114746],\n",
       "        [0.081543, 0.0688477, -0.0986328, ..., 0.0205078, -0.128906,\n",
       "         0.0397949]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [-0.0402832, 0.103516, -0.125977, ..., -0.0463867, -0.120117,\n",
       "         0.0263672],\n",
       "        [-0.0395508, 0.0395508, -0.0510254, ..., -0.0603027, -0.134766,\n",
       "         0.0371094],\n",
       "        [0.0229492, 0.0527344, -0.0510254, ..., 0.0253906, -0.308594,\n",
       "         0.0922852]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [-0.222656, -0.46875, -0.613281, ..., 0.0195312, -0.0415039,\n",
       "         0.171875],\n",
       "        [-0.261719, -0.398438, -0.0957031, ..., -0.0537109, -0.155273,\n",
       "         0.182617],\n",
       "        [-0.232422, -0.265625, -0.188477, ..., 0.09375, -0.241211,\n",
       "         0.232422]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.355469, -0.0291748, -0.414062, ..., -0.21875, -0.308594,\n",
       "         -0.110352],\n",
       "        [0.371094, 0.0996094, 0.212891, ..., -0.394531, -0.429688,\n",
       "         -0.0595703],\n",
       "        [0.316406, 0.291016, 0.0751953, ..., -0.287109, -0.445312,\n",
       "         -0.09375]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0.300781, 0.960938, -0.078125, ..., -0.691406, -1.42188,\n",
       "         0.081543],\n",
       "        [0.328125, 1.09375, 0.273438, ..., -1.00781, -1.49219, 0.148438],\n",
       "        [0.202148, 1.32031, 0.585938, ..., -1.03125, -1.55469, 0.121094]]],      dtype=bfloat16)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_batch_dfs[0].iloc[0][\"model_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid  # Generates unique file keys\n",
    "import csv\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "JAX_STORE_PATH = \"Data/Processed/jax_store/chunk_0\"  # Directory to store pickled JAX arrays\n",
    "\n",
    "os.makedirs(JAX_STORE_PATH, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "def save_jax_array(array):\n",
    "    \"\"\"Save a JAX array to a pickle file and return a unique lookup key.\"\"\"\n",
    "    unique_id = str(uuid.uuid4())  # Generate a unique key (v low collision chance)\n",
    "    file_path = os.path.join(JAX_STORE_PATH, f\"{unique_id}.pkl\")\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(array, f)\n",
    "\n",
    "    return unique_id  # Return lookup key to store in CSV\n",
    "\n",
    "def load_jax_array(key):\n",
    "    \"\"\"Load a JAX array from a pickle file using the given lookup key.\"\"\"\n",
    "    file_path = os.path.join(JAX_STORE_PATH, f\"{key}.pkl\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"JAX array file not found: {file_path}\")\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)  # Load the JAX array\n",
    "\n",
    "def write_row(row, writer, parent):\n",
    "    \"\"\"Writes a single row to the CSV file using the provided CSV writer.\"\"\"\n",
    "    original = parent.iloc[row[\"map_index\"]]\n",
    "\n",
    "    # Save JAX array and store lookup key\n",
    "    model_output_key = save_jax_array(row[\"model_output\"])\n",
    "\n",
    "    to_write = {\n",
    "        \"def_or_sentence\": row[\"column\"],\n",
    "        \"def_index\": row[\"def_index\"],\n",
    "        \"model_output_key\": model_output_key,  # Store only the lookup key\n",
    "        \"word\": original[\"word\"],\n",
    "        \"sentence\": original[\"sentence\"],\n",
    "        \"word_loc\": original[\"word_loc\"],\n",
    "        \"wordnet\": original[\"wordnet\"],\n",
    "        \"definition\": original[\"definition\"],\n",
    "        \"definitions\": original[\"definitions\"]\n",
    "    }\n",
    "\n",
    "    writer.writerow(to_write)\n",
    "\n",
    "def write_results_to_disk(processed_batch_dfs, filePath: str, parent_df: pd.DataFrame):\n",
    "    \"\"\"Writes rows from a list of processed batch DataFrames, storing JAX arrays separately.\"\"\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(filePath), exist_ok=True)\n",
    "\n",
    "    file_exists = os.path.isfile(filePath)\n",
    "\n",
    "    with open(filePath, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"def_or_sentence\", \"def_index\", \"model_output_key\", \"word\", \n",
    "            \"sentence\", \"word_loc\", \"wordnet\", \"definition\", \"definitions\"\n",
    "        ])\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only if file doesn't exist\n",
    "\n",
    "        for processed_batch_df in tqdm(processed_batch_dfs, desc=\"Processing Batches\"):\n",
    "            for _, row in tqdm(processed_batch_df.iterrows(), total=processed_batch_df.shape[0], desc=\"Writing Rows\"):\n",
    "                write_row(row, writer, parent_df)\n",
    "\n",
    "        f.flush()  # Ensure data is written immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write= 'Data/Processed/ProcessedSemCoreChunks/chunk_0.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|                                                                        | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 30.32it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 54.31it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 29.22it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 22.62it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 26.66it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 46.35it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 49.03it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 41.66it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 45.61it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 42.44it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:01<00:00, 44.21it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 64/64 [00:04<00:00, 14.41it/s]\n",
      "Writing Rows: 100%|█████████████████████████████████████████████████████████████████████| 62/62 [00:01<00:00, 43.06it/s]\n",
      "Processing Batches: 100%|███████████████████████████████████████████████████████████████| 13/13 [00:25<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "write_results_to_disk(processed_batch_dfs, to_write, df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_STORE_PATH = \"Data/Processed/jax_store/chunk_0.zip\"\n",
    "\n",
    "def zip_and_clean_jax_store():\n",
    "    \"\"\"Compress the JAX store directory into a zip archive and delete original files.\"\"\"\n",
    "    shutil.make_archive(JAX_STORE_PATH.rstrip(\"/\"), 'zip', JAX_STORE_PATH)  # Create ZIP\n",
    "\n",
    "    # Delete all files inside the jax_store/ directory\n",
    "    for filename in tqdm(os.listdir(JAX_STORE_PATH)):\n",
    "        file_path = os.path.join(JAX_STORE_PATH, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)  # Delete file\n",
    "\n",
    "    if os.path.exists(JAX_STORE_PATH) and not os.listdir(JAX_STORE_PATH):\n",
    "        os.rmdir(JAX_STORE_PATH)  # Remove empty folder\n",
    "\n",
    "    print(f\"✅ JAX store compressed to {ZIP_STORE_PATH} and cleaned up.\")\n",
    "\n",
    "\n",
    "def unzip_jax_store():\n",
    "    \"\"\"Extracts the JAX store archive.\"\"\"\n",
    "    with zipfile.ZipFile(ZIP_STORE_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(JAX_STORE_PATH)\n",
    "    print(f\"✅ JAX store extracted to {JAX_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 830/830 [00:07<00:00, 107.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JAX store compressed to Data/Processed/jax_store/chunk_0.zip and cleaned up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zip_and_clean_jax_store()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
