{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama to Jax\n",
    "\n",
    "In this notebook we test the conversion of the Llama architecture to use Jax as the backend, by isolating the components individually and ensuring that they yield the same results, as function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, norm_eps=1e-05, rope_theta=500000, use_scaled_rope=True, rope_scale_factor=32.0, max_batch_size=32, original_rotary_embed_len=8192, cache_len=2048)\n"
     ]
    }
   ],
   "source": [
    "from llama.model import ModelArgs\n",
    "import json\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    use_scale_rope=True, # this is how it was in llama.ipynb\n",
    "    rope_scale_factor=config.get(\"rope_scaling\").get(\"factor\"),\n",
    "    original_rotary_embed_len=config.get(\"rope_scaling\").get(\"original_max_position_embeddings\"),\n",
    "    cache_len = 2048,\n",
    ")\n",
    "\n",
    "print(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RMSNorm`\n",
    "\n",
    "Here we'll demonstrate how to create jax arrays from the pytorch parameters, and see how our jax implementation of `RMSNorm` compares to the one we already know is a part of the working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded source norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded jax norm\n"
     ]
    }
   ],
   "source": [
    "# load the functions to compare\n",
    "from llama.model import RMSNorm as RMSNorm_pt\n",
    "print(\"loaded source norm\")\n",
    "from llama_jax.model import RMSNorm as RMSNorm_jax\n",
    "print(\"loaded jax norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights to find a good demo tensor\n",
    "[key for key in weights.keys() if 'norm' in key][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-07 15:26:22,043:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: [0.158203 0.180664 0.269531 ... 0.22168 0.210938 0.152344], jax: [0.158203 0.180664 0.269531 ... 0.22168 0.210938 0.152344]\n",
      "pytorch shape: torch.Size([2048]), jax shape: (2048,)\n"
     ]
    }
   ],
   "source": [
    "# pytorch tensor --> jax array\n",
    "rms_weights_pt = weights['model.layers.0.input_layernorm.weight']\n",
    "rms_weights_jax = jnp.array(rms_weights_pt.detach().float().numpy(), dtype=jnp.bfloat16)\n",
    "\n",
    "print(f\"pytorch: {rms_weights_jax}, jax: {rms_weights_jax}\")\n",
    "print(f\"pytorch shape: {rms_weights_pt.shape}, jax shape: {rms_weights_jax.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSNorm()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the weights in an isolated pytorch RMSNorm module\n",
    "rms_norm_pt = RMSNorm_pt(2048)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # overwrite the RMSNorm weight with the one from the loaded state_dict\n",
    "    rms_norm_pt.weight.copy_(rms_weights_pt)\n",
    "\n",
    "rms_norm_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch output: tensor([[-0.0100,  0.1573, -0.4949,  ..., -0.0077,  0.1183,  0.1864],\n",
      "        [-0.0805, -0.2562,  0.1460,  ..., -0.0104,  0.0233,  0.0328]])\n"
     ]
    }
   ],
   "source": [
    "# run the pytorch version on a sample tensor to get a \"true\" value\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_torch = torch.randn(2, 2048) # add a batch dim\n",
    "\n",
    "    y_torch = rms_norm_pt(x_torch)\n",
    "    print(\"PyTorch output:\", y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax output: [[-0.00994873 0.157227 -0.494141 ... -0.00769043 0.118652 0.186523]\n",
      " [-0.0805664 -0.257812 0.146484 ... -0.010376 0.0233154 0.0327148]]\n"
     ]
    }
   ],
   "source": [
    "# call our jax implementation and check the output is the same\n",
    "\n",
    "rms_norm_jax = lambda x : RMSNorm_jax(x, rms_weights_jax)\n",
    "\n",
    "x_jax = jnp.array(x_torch.detach().numpy(), dtype=jnp.bfloat16)\n",
    "y_jax = rms_norm_jax(x_jax)\n",
    "\n",
    "print(\"Jax output:\", y_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all looks good :), the pytorch outputs `bfloat16` too, whereas jax has higher precision. We'll worry about that later, since I don't know whether that is the correct behaviour when the function is just one step in the overall architecture (for the pytorch implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `precompute_freq_cis`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import precompute_freqs_cis as precompute_freqs_cis_pt\n",
    "from llama_jax.model import precompute_freqs_cis as precompute_freqs_cis_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq_cis_pt[1000,:]=tensor([ 0.5624+8.2688e-01j, -0.7484-6.6329e-01j,  0.8558+5.1728e-01j,\n",
      "        -0.9982-5.9692e-02j,  0.6555-7.5522e-01j, -0.9931+1.1765e-01j,\n",
      "        -0.8397-5.4308e-01j,  0.9927+1.2066e-01j,  0.9957-9.2948e-02j,\n",
      "         0.9843-1.7641e-01j, -0.6581-7.5291e-01j, -0.0060-9.9998e-01j,\n",
      "         0.5323+8.4655e-01j,  0.1267-9.9194e-01j, -0.9976-6.9797e-02j,\n",
      "        -0.5315+8.4708e-01j,  0.1559+9.8777e-01j,  0.5910+8.0666e-01j,\n",
      "         0.9998+1.9460e-02j,  0.9999+1.2914e-02j,  1.0000+8.5702e-03j,\n",
      "         1.0000+5.6872e-03j,  1.0000+3.7740e-03j,  1.0000+2.5045e-03j,\n",
      "         1.0000+1.6620e-03j,  1.0000+1.1029e-03j,  1.0000+7.3187e-04j,\n",
      "         1.0000+4.8567e-04j,  1.0000+3.2229e-04j,  1.0000+2.1387e-04j,\n",
      "         1.0000+1.4193e-04j,  1.0000+9.4183e-05j])\n",
      "freq_cis_pt.dtype=torch.complex64\n"
     ]
    }
   ],
   "source": [
    "dim = 2048 // 32 \n",
    "\n",
    "freq_cis_pt = precompute_freqs_cis_pt(dim, 2048, 500_000, True, 32, 8192)\n",
    "print(f\"{freq_cis_pt[1000,:]=}\")\n",
    "print(f\"{freq_cis_pt.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq_cis_jax[1000,:]=Array([ 0.56237906+8.2687956e-01j, -0.7483619 -6.6329068e-01j,\n",
      "        0.85581774+5.1727748e-01j, -0.99821687-5.9691951e-02j,\n",
      "        0.6554753 -7.5521660e-01j, -0.9930554 +1.1764777e-01j,\n",
      "       -0.839681  -5.4307991e-01j,  0.99269414+1.2065805e-01j,\n",
      "        0.995671  -9.2947975e-02j,  0.98431766-1.7640516e-01j,\n",
      "       -0.65812033-7.5291276e-01j, -0.00604559-9.9998170e-01j,\n",
      "        0.53230125+8.4655499e-01j,  0.12669091-9.9194223e-01j,\n",
      "       -0.9975612 -6.9796599e-02j, -0.53146   +8.4708339e-01j,\n",
      "        0.15594384+9.8776591e-01j,  0.59101975+8.0665708e-01j,\n",
      "        0.99981064+1.9460410e-02j,  0.9999166 +1.2914409e-02j,\n",
      "        0.9999633 +8.5701505e-03j,  0.99998385+5.6872014e-03j,\n",
      "        0.9999929 +3.7740455e-03j,  0.99999684+2.5044645e-03j,\n",
      "        0.9999986 +1.6619666e-03j,  0.9999994 +1.1028834e-03j,\n",
      "        0.99999976+7.3187490e-04j,  0.9999999 +4.8567311e-04j,\n",
      "        0.99999994+3.2229329e-04j,  1.        +2.1387423e-04j,\n",
      "        1.        +1.4192720e-04j,  1.        +9.4183066e-05j],      dtype=complex64)\n",
      "freq_cis_jax.dtype=dtype('complex64')\n"
     ]
    }
   ],
   "source": [
    "freq_cis_jax = precompute_freqs_cis_jax(dim, 2048, 500_000, True, 32, 8192)\n",
    "print(f\"{freq_cis_jax[1000,:]=}\")\n",
    "print(f\"{freq_cis_jax.dtype=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(freq_cis_pt.detach().numpy() - np.array(freq_cis_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these match too, since the arrays are bigger, we check by looking at the norm too - which shows they are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `apply_rotary_emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import apply_rotary_emb as apply_rotary_emb_pt\n",
    "from llama_jax.model import apply_rotary_emb as apply_rotary_emb_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up dummy data\n",
    "\n",
    "bsz = 1  # batch size\n",
    "seqlen = 30 # dummy value\n",
    "n_local_heads = 32 # no parallelism so local = total\n",
    "head_dim = 2048 // 32\n",
    "\n",
    "dummy_shape = (bsz, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    freq_cis_pt = precompute_freqs_cis_pt(dim, seqlen, 500_000, True, 32, 8192)\n",
    "    xq_torch = torch.randn(dummy_shape) \n",
    "    xk_torch = torch.randn(dummy_shape)\n",
    "\n",
    "freq_cis_jax = precompute_freqs_cis_jax(dim, seqlen, 500_000, True, 32, 8192)\n",
    "xq_jax = jnp.array(xq_torch.detach().numpy())\n",
    "xk_jax = jnp.array(xk_torch.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yq_torch, yk_torch = apply_rotary_emb_pt(xq_torch, xk_torch, freq_cis_pt)\n",
    "\n",
    "yq_jax, yk_jax = apply_rotary_emb_jax(xq_jax, xk_jax, freq_cis_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yq_torch.shape=torch.Size([1, 30, 32, 64])\n",
      "yq_jax.shape=(1, 30, 32, 64)\n",
      "yk_torch.shape=torch.Size([1, 30, 32, 64])\n",
      "yk_jax.shape=(1, 30, 32, 64)\n",
      "yq_torch.dtype=torch.float32\n",
      "yq_jax.dtype=dtype('float32')\n"
     ]
    }
   ],
   "source": [
    "print(f\"{yq_torch.shape=}\")\n",
    "print(f\"{yq_jax.shape=}\")\n",
    "\n",
    "print(f\"{yk_torch.shape=}\")\n",
    "print(f\"{yk_jax.shape=}\")\n",
    "\n",
    "print(f\"{yq_torch.dtype=}\")\n",
    "print(f\"{yq_jax.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q error: 7.465223916369723e-06\n",
      "k error: 7.636589543835726e-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"q error: {np.linalg.norm(yq_torch.detach().numpy() - np.array(yq_jax))}\")\n",
    "print(f\"k error: {np.linalg.norm(yk_torch.detach().numpy() - np.array(yk_jax))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is a slight difference in the values here, but over the vast amount of dummy data it is well within numerical tolerances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `attention_block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import Attention as Attention_pt\n",
    "from llama_jax.model import attention_block as attention_block_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.10.input_layernorm.weight',\n",
       " 'model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight',\n",
       " 'model.layers.10.post_attention_layernorm.weight',\n",
       " 'model.layers.10.self_attn.k_proj.weight',\n",
       " 'model.layers.10.self_attn.o_proj.weight',\n",
       " 'model.layers.10.self_attn.q_proj.weight',\n",
       " 'model.layers.10.self_attn.v_proj.weight']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in weights.keys() if '10' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch params:\n",
      "    model.layers.10.self_attn.k_proj.weight dtype: torch.bfloat16 shape: torch.Size([512, 2048])\n",
      "    model.layers.10.self_attn.o_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 2048])\n",
      "    model.layers.10.self_attn.q_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 2048])\n",
      "    model.layers.10.self_attn.v_proj.weight dtype: torch.bfloat16 shape: torch.Size([512, 2048])\n",
      "\n",
      "jax params\n",
      "    model.layers.10.self_attn.k_proj.weight dtype: bfloat16 shape: (512, 2048)\n",
      "    model.layers.10.self_attn.o_proj.weight dtype: bfloat16 shape: (2048, 2048)\n",
      "    model.layers.10.self_attn.q_proj.weight dtype: bfloat16 shape: (2048, 2048)\n",
      "    model.layers.10.self_attn.v_proj.weight dtype: bfloat16 shape: (512, 2048)\n"
     ]
    }
   ],
   "source": [
    "# get all the required parameters for one attention block\n",
    "# check that the conversion leaves shape and dtype invariant\n",
    "\n",
    "layer10_keys = [key for key in weights.keys() if ('10' in key) and ('norm' not in key) and ('mlp' not in key)]\n",
    "layer10_weights_pt = { key : weights[key] for key in layer10_keys}\n",
    "layer10_weights_jax = { key : jnp.array(weights[key].detach().float().numpy(), dtype = jnp.bfloat16) for key in layer10_keys}\n",
    "\n",
    "print(\"pytorch params:\")\n",
    "for key, value in layer10_weights_pt.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")\n",
    "\n",
    "print(\"\\njax params\")\n",
    "for key, value in layer10_weights_jax.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    attention_pt = Attention_pt(model_args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    attention_pt.wq.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.q_proj.weight\"])\n",
    "    attention_pt.wk.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.k_proj.weight\"])\n",
    "    attention_pt.wv.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.v_proj.weight\"])\n",
    "    attention_pt.wo.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.o_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x_shape = (bsz, seqlen, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.randn(dummy_x_shape) \n",
    "y_torch = attention_pt.forward(x_torch, 0, freq_cis_pt, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jax = jnp.array(x_torch.detach().numpy(), dtype=jnp.bfloat16)\n",
    "\n",
    "y_jax = attention_block_jax(x_jax, None, freq_cis_jax,\n",
    "    wq = layer10_weights_jax[\"model.layers.10.self_attn.q_proj.weight\"],\n",
    "    wk = layer10_weights_jax[\"model.layers.10.self_attn.k_proj.weight\"],\n",
    "    wv = layer10_weights_jax[\"model.layers.10.self_attn.v_proj.weight\"],\n",
    "    wo = layer10_weights_jax[\"model.layers.10.self_attn.o_proj.weight\"],\n",
    "    n_heads = 32,\n",
    "    n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33409098"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `AttentionBlock` with a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "        -inf, -inf, -inf, -inf, -inf, -inf])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(20) \n",
    "neg_inf = torch.full((10,), float('-inf')) \n",
    "mask_pt = torch.cat([zeros, neg_inf]) \n",
    "mask_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = attention_pt.forward(x_torch, 0, freq_cis_pt, mask_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_jax = jnp.array(mask_pt.detach().numpy())\n",
    "\n",
    "y_jax = attention_block_jax(x_jax, mask_jax, freq_cis_jax,\n",
    "    wq = layer10_weights_jax[\"model.layers.10.self_attn.q_proj.weight\"],\n",
    "    wk = layer10_weights_jax[\"model.layers.10.self_attn.k_proj.weight\"],\n",
    "    wv = layer10_weights_jax[\"model.layers.10.self_attn.v_proj.weight\"],\n",
    "    wo = layer10_weights_jax[\"model.layers.10.self_attn.o_proj.weight\"],\n",
    "    n_heads = 32,\n",
    "    n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3009763"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedForward`\n",
    "\n",
    "This is a special MLP implementation that passes a through a linear product of the values, which tempers the non-linearity of the `silu` activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import FeedForward as FeedForward_pt\n",
    "from llama_jax.model import feed_forward as feed_forward_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in weights.keys() if ('10' in key) and ('mlp' in key)]\n",
    "\n",
    "# down -> w2\n",
    "# up   -> w3\n",
    "# gate -> w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch params:\n",
      "    model.layers.10.mlp.down_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 8192])\n",
      "    model.layers.10.mlp.gate_proj.weight dtype: torch.bfloat16 shape: torch.Size([8192, 2048])\n",
      "    model.layers.10.mlp.up_proj.weight dtype: torch.bfloat16 shape: torch.Size([8192, 2048])\n",
      "\n",
      "jax params\n",
      "    model.layers.10.mlp.down_proj.weight dtype: bfloat16 shape: (2048, 8192)\n",
      "    model.layers.10.mlp.gate_proj.weight dtype: bfloat16 shape: (8192, 2048)\n",
      "    model.layers.10.mlp.up_proj.weight dtype: bfloat16 shape: (8192, 2048)\n"
     ]
    }
   ],
   "source": [
    "layer10_keys = [key for key in weights.keys() if ('10' in key) and ('mlp' in key)]\n",
    "layer10_weights_pt = { key : weights[key] for key in layer10_keys}\n",
    "layer10_weights_jax = { key : jnp.array(weights[key].detach().float().numpy(), dtype = jnp.bfloat16) for key in layer10_keys}\n",
    "\n",
    "print(\"pytorch params:\")\n",
    "for key, value in layer10_weights_pt.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")\n",
    "\n",
    "print(\"\\njax params\")\n",
    "for key, value in layer10_weights_jax.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    feed_forward_pt = FeedForward_pt(2048, 4 * 2048, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feed_forward_pt.w1.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.gate_proj.weight\"])\n",
    "    feed_forward_pt.w2.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.down_proj.weight\"])\n",
    "    feed_forward_pt.w3.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.up_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = feed_forward_pt.forward(x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jax = feed_forward_jax(x_jax,\n",
    "    gate = layer10_weights_jax[\"model.layers.10.mlp.gate_proj.weight\"], \n",
    "    down = layer10_weights_jax[\"model.layers.10.mlp.down_proj.weight\"], \n",
    "    up   = layer10_weights_jax[\"model.layers.10.mlp.up_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8660234"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TransfomerBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import TransformerBlock as TransformerBlock_pt\n",
    "from llama_jax.model import transformer_block as transformer_block_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch params:\n",
      "    model.layers.10.input_layernorm.weight dtype: torch.bfloat16 shape: torch.Size([2048])\n",
      "    model.layers.10.mlp.down_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 8192])\n",
      "    model.layers.10.mlp.gate_proj.weight dtype: torch.bfloat16 shape: torch.Size([8192, 2048])\n",
      "    model.layers.10.mlp.up_proj.weight dtype: torch.bfloat16 shape: torch.Size([8192, 2048])\n",
      "    model.layers.10.post_attention_layernorm.weight dtype: torch.bfloat16 shape: torch.Size([2048])\n",
      "    model.layers.10.self_attn.k_proj.weight dtype: torch.bfloat16 shape: torch.Size([512, 2048])\n",
      "    model.layers.10.self_attn.o_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 2048])\n",
      "    model.layers.10.self_attn.q_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 2048])\n",
      "    model.layers.10.self_attn.v_proj.weight dtype: torch.bfloat16 shape: torch.Size([512, 2048])\n",
      "\n",
      "jax params\n",
      "    model.layers.10.input_layernorm.weight dtype: bfloat16 shape: (2048,)\n",
      "    model.layers.10.mlp.down_proj.weight dtype: bfloat16 shape: (2048, 8192)\n",
      "    model.layers.10.mlp.gate_proj.weight dtype: bfloat16 shape: (8192, 2048)\n",
      "    model.layers.10.mlp.up_proj.weight dtype: bfloat16 shape: (8192, 2048)\n",
      "    model.layers.10.post_attention_layernorm.weight dtype: bfloat16 shape: (2048,)\n",
      "    model.layers.10.self_attn.k_proj.weight dtype: bfloat16 shape: (512, 2048)\n",
      "    model.layers.10.self_attn.o_proj.weight dtype: bfloat16 shape: (2048, 2048)\n",
      "    model.layers.10.self_attn.q_proj.weight dtype: bfloat16 shape: (2048, 2048)\n",
      "    model.layers.10.self_attn.v_proj.weight dtype: bfloat16 shape: (512, 2048)\n"
     ]
    }
   ],
   "source": [
    "layer10_keys = [key for key in weights.keys() if ('10' in key)]\n",
    "layer10_weights_pt = { key : weights[key] for key in layer10_keys}\n",
    "layer10_weights_jax = { key : jnp.array(weights[key].detach().float().numpy(), dtype = jnp.bfloat16) for key in layer10_keys}\n",
    "\n",
    "print(\"pytorch params:\")\n",
    "for key, value in layer10_weights_pt.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")\n",
    "\n",
    "print(\"\\njax params\")\n",
    "for key, value in layer10_weights_jax.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    transformer_block_pt = TransformerBlock_pt(10, model_args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    transformer_block_pt.attention_norm.weight.copy_(layer10_weights_pt[\"model.layers.10.input_layernorm.weight\"])\n",
    "\n",
    "    transformer_block_pt.attention.wq.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.q_proj.weight\"])\n",
    "    transformer_block_pt.attention.wk.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.k_proj.weight\"])\n",
    "    transformer_block_pt.attention.wv.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.v_proj.weight\"])\n",
    "    transformer_block_pt.attention.wo.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.o_proj.weight\"])\n",
    "\n",
    "    transformer_block_pt.feed_forward.w1.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.gate_proj.weight\"])\n",
    "    transformer_block_pt.feed_forward.w2.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.down_proj.weight\"])\n",
    "    transformer_block_pt.feed_forward.w3.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.up_proj.weight\"])\n",
    "\n",
    "    transformer_block_pt.ffn_norm.weight.copy_(layer10_weights_pt[\"model.layers.10.post_attention_layernorm.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_torch = transformer_block_pt.forward(x_torch, 0, freq_cis_pt, mask_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the jax param pytree - will need to write a script to do this automatically for the saved weights, once the full architecture is translated\n",
    "\n",
    "attention_params = {\n",
    "    \"wq\" : layer10_weights_jax[\"model.layers.10.self_attn.q_proj.weight\"],\n",
    "    \"wk\" : layer10_weights_jax[\"model.layers.10.self_attn.k_proj.weight\"],\n",
    "    \"wv\" : layer10_weights_jax[\"model.layers.10.self_attn.v_proj.weight\"],\n",
    "    \"wo\" : layer10_weights_jax[\"model.layers.10.self_attn.o_proj.weight\"]\n",
    "}\n",
    "\n",
    "ff_params = {\n",
    "    \"up\"   : layer10_weights_jax[\"model.layers.10.mlp.up_proj.weight\"],\n",
    "    \"gate\" : layer10_weights_jax[\"model.layers.10.mlp.gate_proj.weight\"], \n",
    "    \"down\" : layer10_weights_jax[\"model.layers.10.mlp.down_proj.weight\"]\n",
    "}\n",
    "\n",
    "norm_params = {\n",
    "    \"pre_attention_rms\"  : layer10_weights_jax[\"model.layers.10.input_layernorm.weight\"],\n",
    "    \"post_attention_rms\" : layer10_weights_jax[\"model.layers.10.post_attention_layernorm.weight\"]\n",
    "}\n",
    "\n",
    "param_pytree = {\n",
    "    \"attention\"    : attention_params,\n",
    "    \"feed_forward\" : ff_params,\n",
    "    \"norms\"        : norm_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.640625, -0.613281, 0.527344, ..., 1.46094, -0.714844,\n",
       "         0.785156],\n",
       "        [0.371094, 0.0722656, -0.263672, ..., -0.238281, 0.00637817,\n",
       "         0.511719],\n",
       "        [-2.01562, -0.447266, -1.26562, ..., 1.00781, -0.578125,\n",
       "         1.03906],\n",
       "        ...,\n",
       "        [-0.166016, -0.988281, 0.15918, ..., -1.33594, -0.746094,\n",
       "         -0.582031],\n",
       "        [-0.546875, -0.863281, 0.550781, ..., 0.839844, -1.03906,\n",
       "         -1.41406],\n",
       "        [1.4375, 1.17188, 0.964844, ..., 0.486328, 0.652344, 1.98438]]],      dtype=bfloat16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jax = transformer_block_jax(x_jax,\n",
    "    param_pytree,\n",
    "    mask_jax, freq_cis_jax,\n",
    "    n_heads = 32, n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.757297"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're starting to see some drift here, but given that there is still a difference in the precision used, I think this is not a dealbreaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Transformer`\n",
    "\n",
    "**NOTE** - tokens usually have a **preceding** space, adding a space at the end of the prompt really messes things up, and we get weird predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import Transformer as Transformer_pt\n",
    "from llama_jax.model import transformer as transformer_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n"
     ]
    }
   ],
   "source": [
    "transformer_pt = Transformer_pt(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_state_dict = {}\n",
    "\n",
    "for key in weights.keys():\n",
    "    \n",
    "    new_key = key\n",
    "    new_key = new_key.replace(\"model.\", \"\")\n",
    "    \n",
    "    new_key = new_key.replace(\"embed_tokens.weight\", \"tok_embeddings.weight\")\n",
    "\n",
    "    new_key = new_key.replace(\"self_attn.q_proj\", \"attention.wq\")\n",
    "    new_key = new_key.replace(\"self_attn.k_proj\", \"attention.wk\")\n",
    "    new_key = new_key.replace(\"self_attn.v_proj\", \"attention.wv\")\n",
    "    new_key = new_key.replace(\"self_attn.o_proj\", \"attention.wo\")\n",
    "\n",
    "    new_key = new_key.replace(\"mlp.gate_proj\", \"feed_forward.w1\")\n",
    "    new_key = new_key.replace(\"mlp.up_proj\", \"feed_forward.w3\")\n",
    "    new_key = new_key.replace(\"mlp.down_proj\", \"feed_forward.w2\")\n",
    "\n",
    "    new_key = new_key.replace(\"input_layernorm\", \"attention_norm\")\n",
    "    new_key = new_key.replace(\"post_attention_layernorm\", \"ffn_norm\")\n",
    "\n",
    "    new_key = new_key.replace(\"model.norm\", \"norm\")\n",
    "\n",
    "    fixed_state_dict[new_key] = weights[key]\n",
    "\n",
    "fixed_state_dict[\"output.weight\"] = fixed_state_dict[\"tok_embeddings.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_pt.load_state_dict(fixed_state_dict, strict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7febf3f284a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "tok = Tokenizer(tok_path)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toks=[128000, 791, 6864, 315, 9822, 374]\n",
      "tok.decode(toks)='<|begin_of_text|>The capital of France is'\n"
     ]
    }
   ],
   "source": [
    "toks = tok.encode(\"The capital of France is\", bos=True, eos=False)\n",
    "print(f\"{toks=}\")\n",
    "print(f\"{tok.decode(toks)=}\")\n",
    "\n",
    "toks = torch.tensor(toks, dtype=torch.int).unsqueeze(0) # dummy batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [ 5.3197,  6.3458,  5.5625,  ..., -0.9394, -0.9385, -0.9387],\n",
       "         [ 9.4542,  7.4223,  5.0388,  ..., -0.4380, -0.4379, -0.4378],\n",
       "         [ 8.4220,  8.3505,  5.5084,  ...,  0.1847,  0.1850,  0.1848],\n",
       "         [14.8629, 12.0516,  9.5194,  ...,  0.8880,  0.8878,  0.8880],\n",
       "         [10.2351,  8.6261,  4.6638,  ...,  0.8458,  0.8463,  0.8458]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_torch = transformer_pt(toks, 0)\n",
    "out_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12366]\n"
     ]
    }
   ],
   "source": [
    "print(tok.encode(\" Paris\", bos=False, eos = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Token ' the' with log probability -2.0233542919158936\n",
      "Rank 2: Token ' Paris' with log probability -2.0792510509490967\n",
      "Rank 3: Token ' a' with log probability -2.3678767681121826\n",
      "Rank 4: Token ' located' with log probability -2.5045363903045654\n",
      "Rank 5: Token ' situated' with log probability -2.869053602218628\n",
      "Rank 6: Token ' in' with log probability -3.737622022628784\n",
      "Rank 7: Token ' known' with log probability -3.8099029064178467\n",
      "Rank 8: Token ' one' with log probability -3.9084270000457764\n",
      "Rank 9: Token ' not' with log probability -4.317896366119385\n",
      "Rank 10: Token ' called' with log probability -4.401501178741455\n",
      "\n",
      "Next token is: ' the'\n",
      "Token ' Paris' with log probability -2.0792510509490967\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(out_torch[0, -1, :], dim=0)\n",
    "\n",
    "top_k = 10\n",
    "top_probs, top_tokens = torch.topk(probs, top_k)\n",
    "\n",
    "log_probs = torch.log(top_probs)\n",
    "\n",
    "for i in range(top_k):\n",
    "    token_str = tok.decode([top_tokens[i].item()])\n",
    "    print(f\"Rank {i+1}: Token '{token_str}' with log probability {log_probs[i].item()}\")\n",
    "\n",
    "next_token = top_tokens[0]\n",
    "print(f\"\\nNext token is: '{tok.decode([next_token.item()])}'\")\n",
    "\n",
    "token_str = \" Paris\"\n",
    "print(f\"Token '{token_str}' with log probability {torch.log(probs[12366]).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_jax = lambda pt : jnp.array(pt.detach().float().numpy(), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embed_jax = to_jax(weights[\"model.embed_tokens.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:41<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "layer_weights = []\n",
    "\n",
    "keys_left = weights.keys()\n",
    "\n",
    "# reverse the range because we'll remove keys we have seen already\n",
    "# this will help because e.g. 1 is in 15, but 15 is not in 1 \n",
    "\n",
    "layer_params = []\n",
    "\n",
    "for i in tqdm(reversed(range(num_layers)), total = num_layers):\n",
    "\n",
    "    layer_keys = [key for key in keys_left if (str(i) in key)]\n",
    "    keys_left  = list(set(keys_left) - set(layer_keys))\n",
    "    \n",
    "    attention_params = {\n",
    "        \"wq\" : to_jax(weights[f\"model.layers.{i}.self_attn.q_proj.weight\"]),\n",
    "        \"wk\" : to_jax(weights[f\"model.layers.{i}.self_attn.k_proj.weight\"]),\n",
    "        \"wv\" : to_jax(weights[f\"model.layers.{i}.self_attn.v_proj.weight\"]),\n",
    "        \"wo\" : to_jax(weights[f\"model.layers.{i}.self_attn.o_proj.weight\"])\n",
    "    }\n",
    "\n",
    "    ff_params = {\n",
    "        \"up\"   : to_jax(weights[f\"model.layers.{i}.mlp.up_proj.weight\"]),\n",
    "        \"gate\" : to_jax(weights[f\"model.layers.{i}.mlp.gate_proj.weight\"]), \n",
    "        \"down\" : to_jax(weights[f\"model.layers.{i}.mlp.down_proj.weight\"])\n",
    "    }\n",
    "\n",
    "    norm_params = {\n",
    "        \"pre_attention_rms\"  : to_jax(weights[f\"model.layers.{i}.input_layernorm.weight\"]),\n",
    "        \"post_attention_rms\" : to_jax(weights[f\"model.layers.{i}.post_attention_layernorm.weight\"])\n",
    "    }\n",
    "\n",
    "    param_pytree = {\n",
    "        \"attention\"    : attention_params,\n",
    "        \"feed_forward\" : ff_params,\n",
    "        \"norms\"        : norm_params\n",
    "    }\n",
    "\n",
    "    layer_params.append(param_pytree)\n",
    "\n",
    "layer_params.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_jax = {\n",
    "    \"tok_embeddings\" : to_jax(weights[\"model.embed_tokens.weight\"]),\n",
    "    \"freqs_cis\"      : freq_cis_jax,\n",
    "    \"layers\"         : layer_params,\n",
    "    \"norm_scale\"     : to_jax(weights[\"model.norm.weight\"]),\n",
    "    \"output_weight\"  : to_jax(weights[\"model.embed_tokens.weight\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_cis_jax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(toks)=30\n"
     ]
    }
   ],
   "source": [
    "toks = tok.encode(\"The capital of France is\", bos=True, eos=False)\n",
    "toks = toks + [0] * (freq_cis_jax.shape[0] - len(toks))\n",
    "\n",
    "print(f\"{len(toks)=}\") # pad out since we need the precomputed freq_cis to match the number of tokens\n",
    "\n",
    "mask = [0 if tok != 0 else -jnp.inf for tok in toks] # mask the pad tokens\n",
    "\n",
    "toks = jnp.array(toks)[None, :] # add dummy \"batch\" axis\n",
    "mask = jnp.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_jax = transformer_jax(toks, params_jax, mask, n_heads = 32, n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Token 'The' with log probability -3.1875\n",
      "Rank 2: Token ' The' with log probability -3.25\n",
      "Rank 3: Token 'A' with log probability -3.875\n",
      "Rank 4: Token ' I' with log probability -3.875\n",
      "Rank 5: Token 'France' with log probability -3.875\n",
      "Rank 6: Token ' This' with log probability -4.125\n",
      "Rank 7: Token 'This' with log probability -4.25\n",
      "Rank 8: Token ' ' with log probability -4.4375\n",
      "Rank 9: Token ' It' with log probability -4.4375\n",
      "Rank 10: Token ' France' with log probability -4.4375\n",
      "Rank 11: Token ' A' with log probability -4.5\n",
      "Rank 12: Token ' ' with log probability -4.5625\n",
      "Rank 13: Token 'I' with log probability -4.6875\n",
      "Rank 14: Token 'In' with log probability -4.6875\n",
      "Rank 15: Token 'It' with log probability -4.75\n",
      "Rank 16: Token 'French' with log probability -4.875\n",
      "Rank 17: Token ' In' with log probability -4.9375\n",
      "Rank 18: Token ' and' with log probability -5.0\n",
      "Rank 19: Token ' We' with log probability -5.0\n",
      "Rank 20: Token 'Paris' with log probability -5.0625\n",
      "Rank 21: Token 'We' with log probability -5.125\n",
      "Rank 22: Token ' (' with log probability -5.25\n",
      "Rank 23: Token ' is' with log probability -5.25\n",
      "Rank 24: Token ' the' with log probability -5.3125\n",
      "Rank 25: Token ' If' with log probability -5.3125\n",
      "\n",
      "Next token is: 'The'\n",
      "Token ' Paris' with log probability -5.6875\n"
     ]
    }
   ],
   "source": [
    "probs = jax.nn.softmax(out_jax[0, -1, :], axis=0)\n",
    "\n",
    "top_k = 25\n",
    "top_probs, top_tokens = jax.lax.top_k(probs, top_k)\n",
    "\n",
    "log_probs = jnp.log(top_probs)\n",
    "\n",
    "for i in range(top_k):\n",
    "    token_str = tok.decode([top_tokens[i].item()])\n",
    "    print(f\"Rank {i+1}: Token '{token_str}' with log probability {log_probs[i].item()}\")\n",
    "\n",
    "next_token = top_tokens[0]\n",
    "print(f\"\\nNext token is: '{tok.decode([next_token.item()])}'\")\n",
    "\n",
    "token_str = \" Paris\"\n",
    "print(f\"Token '{token_str}' with log probability {jnp.log(probs[12366]).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953.33966"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array(out_jax[0, -1, :]) - out_torch[0, -1, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.8125, 4.53125, 12.0625, ..., -0.486328, -0.486328, -0.486328],\n",
       "      dtype=bfloat16)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(out_jax[0, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.23513   ,  8.626071  ,  4.6638365 , ...,  0.8458147 ,\n",
       "        0.846267  ,  0.84582824], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_torch[0, -1, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a significant degredation as the errors compound through the model - however the desired token still does appear in the prediction list. This is likely to do with the datatypes used througout - as the bfloat 16 can be hurting the performances. And there may be underlying implementation differences in the Jax vs pytorch routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just in time compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_transformer = jax.jit(transformer_jax, static_argnames=[\"n_heads\", \"n_kv_heads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[9.625, 12.0625, 14.1875, ..., -5.15625, -5.15625, -5.15625],\n",
       "        [6.0625, 9.125, 6.9375, ..., -0.570312, -0.570312, -0.570312],\n",
       "        [12.6875, 10.4375, 7.25, ..., 0.351562, 0.351562, 0.351562],\n",
       "        ...,\n",
       "        [7.8125, 4.34375, 12.3125, ..., -1.23438, -1.23438, -1.23438],\n",
       "        [8.5, 4.59375, 12.125, ..., -0.847656, -0.847656, -0.847656],\n",
       "        [8.875, 4.59375, 12.0625, ..., -0.507812, -0.507812, -0.507812]]],      dtype=bfloat16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitted_transformer(toks, params_jax, mask, n_heads = 32, n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[9.625, 12.0625, 14.1875, ..., -5.15625, -5.15625, -5.15625],\n",
       "        [6.0625, 9.125, 6.9375, ..., -0.570312, -0.570312, -0.570312],\n",
       "        [12.6875, 10.4375, 7.25, ..., 0.351562, 0.351562, 0.351562],\n",
       "        ...,\n",
       "        [7.8125, 4.34375, 12.3125, ..., -1.23438, -1.23438, -1.23438],\n",
       "        [8.5, 4.59375, 12.125, ..., -0.847656, -0.847656, -0.847656],\n",
       "        [8.875, 4.59375, 12.0625, ..., -0.507812, -0.507812, -0.507812]]],      dtype=bfloat16)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitted_transformer(toks, params_jax, mask, n_heads = 32, n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that doing just in time compilation can roughly halve the time it takes to run the model in jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Whilst the Jax implementation does leave a lot to be desired, we can see that it does roughly match the pytorch. We aren't intending to use it for generative purposes, just as a way to access the hidden layers' values.\n",
    "\n",
    "The functional implementation exposes the logic more cleanly and hopefully will make further analysis easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
