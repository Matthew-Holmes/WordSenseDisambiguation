{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama to Jax\n",
    "\n",
    "In this notebook we test the conversion of the Llama architecture to use Jax as the backend, by isolating the components individually and ensuring that they yield the same results, as function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, norm_eps=1e-05, rope_theta=500000, use_scaled_rope=True, rope_scale_factor=32.0, max_batch_size=32, original_rotary_embed_len=8192, cache_len=2048)\n"
     ]
    }
   ],
   "source": [
    "from llama.model import ModelArgs\n",
    "import json\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    use_scale_rope=True, # this is how it was in llama.ipynb\n",
    "    rope_scale_factor=config.get(\"rope_scaling\").get(\"factor\"),\n",
    "    original_rotary_embed_len=config.get(\"rope_scaling\").get(\"original_max_position_embeddings\"),\n",
    "    cache_len = 2048,\n",
    ")\n",
    "\n",
    "print(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RMSNorm`\n",
    "\n",
    "Here we'll demonstrate how to create jax arrays from the pytorch parameters, and see how our jax implementation of `RMSNorm` compares to the one we already know is a part of the working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded source norm\n",
      "loaded jax norm\n"
     ]
    }
   ],
   "source": [
    "# load the functions to compare\n",
    "from llama.model import RMSNorm as RMSNorm_pt\n",
    "print(\"loaded source norm\")\n",
    "from llama_jax.model import RMSNorm as RMSNorm_jax\n",
    "print(\"loaded jax norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights to find a good demo tensor\n",
    "[key for key in weights.keys() if 'norm' in key][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-05 17:32:50,795:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: [0.158203 0.180664 0.269531 ... 0.22168 0.210938 0.152344], jax: [0.158203 0.180664 0.269531 ... 0.22168 0.210938 0.152344]\n",
      "pytorch shape: torch.Size([2048]), jax shape: (2048,)\n"
     ]
    }
   ],
   "source": [
    "# pytorch tensor --> jax array\n",
    "rms_weights_pt = weights['model.layers.0.input_layernorm.weight']\n",
    "rms_weights_jax = jnp.array(rms_weights_pt.detach().float().numpy(), dtype=jnp.bfloat16)\n",
    "\n",
    "print(f\"pytorch: {rms_weights_jax}, jax: {rms_weights_jax}\")\n",
    "print(f\"pytorch shape: {rms_weights_pt.shape}, jax shape: {rms_weights_jax.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSNorm()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the weights in an isolated pytorch RMSNorm module\n",
    "rms_norm_pt = RMSNorm_pt(2048)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # overwrite the RMSNorm weight with the one from the loaded state_dict\n",
    "    rms_norm_pt.weight.copy_(rms_weights_pt)\n",
    "\n",
    "rms_norm_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch output: tensor([[-0.2999,  0.0068, -0.6558,  ..., -0.4704,  0.0800, -0.0023],\n",
      "        [-0.0136,  0.0844, -0.1716,  ..., -0.0436, -0.2050,  0.0508]])\n"
     ]
    }
   ],
   "source": [
    "# run the pytorch version on a sample tensor to get a \"true\" value\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_torch = torch.randn(2, 2048) # add a batch dim\n",
    "\n",
    "    y_torch = rms_norm_pt(x_torch)\n",
    "    print(\"PyTorch output:\", y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax output: [[-0.29990137  0.00675914 -0.65584993 ... -0.470414    0.07996918\n",
      "  -0.00227949]\n",
      " [-0.0135715   0.0843863  -0.17162883 ... -0.04360401 -0.20500495\n",
      "   0.05083428]]\n"
     ]
    }
   ],
   "source": [
    "# call our jax implementation and check the output is the same\n",
    "\n",
    "rms_norm_jax = lambda x : RMSNorm_jax(x, rms_weights_jax)\n",
    "\n",
    "x_jax = jnp.array(x_torch.detach().numpy())\n",
    "y_jax = rms_norm_jax(x_jax)\n",
    "\n",
    "print(\"Jax output:\", y_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all looks good :), the pytorch outputs `bfloat16` too, whereas jax has higher precision. We'll worry about that later, since I don't know whether that is the correct behaviour when the function is just one step in the overall architecture (for the pytorch implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `precompute_freq_cis`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import precompute_freqs_cis as precompute_freqs_cis_pt\n",
    "from llama_jax.model import precompute_freqs_cis as precompute_freqs_cis_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq_cis_pt[1000,:]=tensor([ 0.5624+8.2688e-01j, -0.7484-6.6329e-01j,  0.8558+5.1728e-01j,\n",
      "        -0.9982-5.9692e-02j,  0.6555-7.5522e-01j, -0.9931+1.1765e-01j,\n",
      "        -0.8397-5.4308e-01j,  0.9927+1.2066e-01j,  0.9957-9.2948e-02j,\n",
      "         0.9843-1.7641e-01j, -0.6581-7.5291e-01j, -0.0060-9.9998e-01j,\n",
      "         0.5323+8.4655e-01j,  0.1267-9.9194e-01j, -0.9976-6.9797e-02j,\n",
      "        -0.5315+8.4708e-01j,  0.1559+9.8777e-01j,  0.5910+8.0666e-01j,\n",
      "         0.9998+1.9460e-02j,  0.9999+1.2914e-02j,  1.0000+8.5702e-03j,\n",
      "         1.0000+5.6872e-03j,  1.0000+3.7740e-03j,  1.0000+2.5045e-03j,\n",
      "         1.0000+1.6620e-03j,  1.0000+1.1029e-03j,  1.0000+7.3187e-04j,\n",
      "         1.0000+4.8567e-04j,  1.0000+3.2229e-04j,  1.0000+2.1387e-04j,\n",
      "         1.0000+1.4193e-04j,  1.0000+9.4183e-05j])\n",
      "freq_cis_pt.dtype=torch.complex64\n"
     ]
    }
   ],
   "source": [
    "dim = 2048 // 32 \n",
    "\n",
    "freq_cis_pt = precompute_freqs_cis_pt(dim, 2048, 500_000, True, 32, 8192)\n",
    "print(f\"{freq_cis_pt[1000,:]=}\")\n",
    "print(f\"{freq_cis_pt.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq_cis_jax[1000,:]=Array([ 0.56237906+8.2687956e-01j, -0.7483619 -6.6329068e-01j,\n",
      "        0.85581774+5.1727748e-01j, -0.99821687-5.9691951e-02j,\n",
      "        0.6554753 -7.5521660e-01j, -0.9930554 +1.1764777e-01j,\n",
      "       -0.839681  -5.4307991e-01j,  0.99269414+1.2065805e-01j,\n",
      "        0.995671  -9.2947975e-02j,  0.98431766-1.7640516e-01j,\n",
      "       -0.65812033-7.5291276e-01j, -0.00604559-9.9998170e-01j,\n",
      "        0.53230125+8.4655499e-01j,  0.12669091-9.9194223e-01j,\n",
      "       -0.9975612 -6.9796599e-02j, -0.53146   +8.4708339e-01j,\n",
      "        0.15594384+9.8776591e-01j,  0.59101975+8.0665708e-01j,\n",
      "        0.99981064+1.9460410e-02j,  0.9999166 +1.2914409e-02j,\n",
      "        0.9999633 +8.5701505e-03j,  0.99998385+5.6872014e-03j,\n",
      "        0.9999929 +3.7740455e-03j,  0.99999684+2.5044645e-03j,\n",
      "        0.9999986 +1.6619666e-03j,  0.9999994 +1.1028834e-03j,\n",
      "        0.99999976+7.3187490e-04j,  0.9999999 +4.8567311e-04j,\n",
      "        0.99999994+3.2229329e-04j,  1.        +2.1387423e-04j,\n",
      "        1.        +1.4192720e-04j,  1.        +9.4183066e-05j],      dtype=complex64)\n",
      "freq_cis_jax.dtype=dtype('complex64')\n"
     ]
    }
   ],
   "source": [
    "freq_cis_jax = precompute_freqs_cis_jax(dim, 2048, 500_000, True, 32, 8192)\n",
    "print(f\"{freq_cis_jax[1000,:]=}\")\n",
    "print(f\"{freq_cis_jax.dtype=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(freq_cis_pt.detach().numpy() - np.array(freq_cis_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these match too, since the arrays are bigger, we check by looking at the norm too - which shows they are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `apply_rotary_emb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import apply_rotary_emb as apply_rotary_emb_pt\n",
    "from llama_jax.model import apply_rotary_emb as apply_rotary_emb_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up dummy data\n",
    "\n",
    "bsz = 1  # batch size\n",
    "seqlen = 30 # dummy value\n",
    "n_local_heads = 32 # no parallelism so local = total\n",
    "head_dim = 2048 // 32\n",
    "\n",
    "dummy_shape = (bsz, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    freq_cis_pt = precompute_freqs_cis_pt(dim, seqlen, 500_000, True, 32, 8192)\n",
    "    xq_torch = torch.randn(dummy_shape) \n",
    "    xk_torch = torch.randn(dummy_shape)\n",
    "\n",
    "freq_cis_jax = precompute_freqs_cis_jax(dim, seqlen, 500_000, True, 32, 8192)\n",
    "xq_jax = jnp.array(xq_torch.detach().numpy())\n",
    "xk_jax = jnp.array(xk_torch.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yq_torch, yk_torch = apply_rotary_emb_pt(xq_torch, xk_torch, freq_cis_pt)\n",
    "\n",
    "yq_jax, yk_jax = apply_rotary_emb_jax(xq_jax, xk_jax, freq_cis_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yq_torch.shape=torch.Size([1, 30, 32, 64])\n",
      "yq_jax.shape=(1, 30, 32, 64)\n",
      "yk_torch.shape=torch.Size([1, 30, 32, 64])\n",
      "yk_jax.shape=(1, 30, 32, 64)\n",
      "yq_torch.dtype=torch.float32\n",
      "yq_jax.dtype=dtype('float32')\n"
     ]
    }
   ],
   "source": [
    "print(f\"{yq_torch.shape=}\")\n",
    "print(f\"{yq_jax.shape=}\")\n",
    "\n",
    "print(f\"{yk_torch.shape=}\")\n",
    "print(f\"{yk_jax.shape=}\")\n",
    "\n",
    "print(f\"{yq_torch.dtype=}\")\n",
    "print(f\"{yq_jax.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q error: 7.464219379471615e-06\n",
      "k error: 7.546961569460109e-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"q error: {np.linalg.norm(yq_torch.detach().numpy() - np.array(yq_jax))}\")\n",
    "print(f\"k error: {np.linalg.norm(yk_torch.detach().numpy() - np.array(yk_jax))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is a slight difference in the values here, but over the vast amount of dummy data it is well within numerical tolerances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `attention_block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import Attention as Attention_pt\n",
    "from llama_jax.model import attention_block as attention_block_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.10.input_layernorm.weight',\n",
       " 'model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight',\n",
       " 'model.layers.10.post_attention_layernorm.weight',\n",
       " 'model.layers.10.self_attn.k_proj.weight',\n",
       " 'model.layers.10.self_attn.o_proj.weight',\n",
       " 'model.layers.10.self_attn.q_proj.weight',\n",
       " 'model.layers.10.self_attn.v_proj.weight']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in weights.keys() if '10' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch params:\n",
      "    model.layers.10.self_attn.k_proj.weight dtype: torch.bfloat16 shape: torch.Size([512, 2048])\n",
      "    model.layers.10.self_attn.o_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 2048])\n",
      "    model.layers.10.self_attn.q_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 2048])\n",
      "    model.layers.10.self_attn.v_proj.weight dtype: torch.bfloat16 shape: torch.Size([512, 2048])\n",
      "\n",
      "jax params\n",
      "    model.layers.10.self_attn.k_proj.weight dtype: bfloat16 shape: (512, 2048)\n",
      "    model.layers.10.self_attn.o_proj.weight dtype: bfloat16 shape: (2048, 2048)\n",
      "    model.layers.10.self_attn.q_proj.weight dtype: bfloat16 shape: (2048, 2048)\n",
      "    model.layers.10.self_attn.v_proj.weight dtype: bfloat16 shape: (512, 2048)\n"
     ]
    }
   ],
   "source": [
    "# get all the required parameters for one attention block\n",
    "# check that the conversion leaves shape and dtype invariant\n",
    "\n",
    "layer10_keys = [key for key in weights.keys() if ('10' in key) and ('norm' not in key) and ('mlp' not in key)]\n",
    "layer10_weights_pt = { key : weights[key] for key in layer10_keys}\n",
    "layer10_weights_jax = { key : jnp.array(weights[key].detach().float().numpy(), dtype = jnp.bfloat16) for key in layer10_keys}\n",
    "\n",
    "print(\"pytorch params:\")\n",
    "for key, value in layer10_weights_pt.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")\n",
    "\n",
    "print(\"\\njax params\")\n",
    "for key, value in layer10_weights_jax.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    attention_pt = Attention_pt(model_args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    attention_pt.wq.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.q_proj.weight\"])\n",
    "    attention_pt.wk.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.k_proj.weight\"])\n",
    "    attention_pt.wv.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.v_proj.weight\"])\n",
    "    attention_pt.wo.weight.copy_(layer10_weights_pt[\"model.layers.10.self_attn.o_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x_shape = (bsz, seqlen, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.randn(dummy_x_shape) \n",
    "y_torch = attention_pt.forward(x_torch, 0, freq_cis_pt, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jax = jnp.array(x_torch.detach().numpy())\n",
    "\n",
    "y_jax = attention_block_jax(x_jax, None, freq_cis_jax,\n",
    "    wq = layer10_weights_jax[\"model.layers.10.self_attn.q_proj.weight\"],\n",
    "    wk = layer10_weights_jax[\"model.layers.10.self_attn.k_proj.weight\"],\n",
    "    wv = layer10_weights_jax[\"model.layers.10.self_attn.v_proj.weight\"],\n",
    "    wo = layer10_weights_jax[\"model.layers.10.self_attn.o_proj.weight\"],\n",
    "    n_heads = 32,\n",
    "    n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3401127e-05"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `AttentionBlock` with a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "        -inf, -inf, -inf, -inf, -inf, -inf])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(20) \n",
    "neg_inf = torch.full((10,), float('-inf')) \n",
    "mask_pt = torch.cat([zeros, neg_inf]) \n",
    "mask_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = attention_pt.forward(x_torch, 0, freq_cis_pt, mask_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_jax = jnp.array(mask_pt.detach().numpy())\n",
    "\n",
    "y_jax = attention_block_jax(x_jax, mask_jax, freq_cis_jax,\n",
    "    wq = layer10_weights_jax[\"model.layers.10.self_attn.q_proj.weight\"],\n",
    "    wk = layer10_weights_jax[\"model.layers.10.self_attn.k_proj.weight\"],\n",
    "    wv = layer10_weights_jax[\"model.layers.10.self_attn.v_proj.weight\"],\n",
    "    wo = layer10_weights_jax[\"model.layers.10.self_attn.o_proj.weight\"],\n",
    "    n_heads = 32,\n",
    "    n_kv_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4069857e-05"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedForward`\n",
    "\n",
    "This is a special MLP implementation that passes a through a linear product of the values, which tempers the non-linearity of the `silu` activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import FeedForward as FeedForward_pt\n",
    "from llama_jax.model import feed_forward as feed_forward_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in weights.keys() if ('10' in key) and ('mlp' in key)]\n",
    "\n",
    "# down -> w2\n",
    "# up   -> w3\n",
    "# gate -> w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch params:\n",
      "    model.layers.10.mlp.down_proj.weight dtype: torch.bfloat16 shape: torch.Size([2048, 8192])\n",
      "    model.layers.10.mlp.gate_proj.weight dtype: torch.bfloat16 shape: torch.Size([8192, 2048])\n",
      "    model.layers.10.mlp.up_proj.weight dtype: torch.bfloat16 shape: torch.Size([8192, 2048])\n",
      "\n",
      "jax params\n",
      "    model.layers.10.mlp.down_proj.weight dtype: bfloat16 shape: (2048, 8192)\n",
      "    model.layers.10.mlp.gate_proj.weight dtype: bfloat16 shape: (8192, 2048)\n",
      "    model.layers.10.mlp.up_proj.weight dtype: bfloat16 shape: (8192, 2048)\n"
     ]
    }
   ],
   "source": [
    "layer10_keys = [key for key in weights.keys() if ('10' in key) and ('mlp' in key)]\n",
    "layer10_weights_pt = { key : weights[key] for key in layer10_keys}\n",
    "layer10_weights_jax = { key : jnp.array(weights[key].detach().float().numpy(), dtype = jnp.bfloat16) for key in layer10_keys}\n",
    "\n",
    "print(\"pytorch params:\")\n",
    "for key, value in layer10_weights_pt.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")\n",
    "\n",
    "print(\"\\njax params\")\n",
    "for key, value in layer10_weights_jax.items():\n",
    "    print(f\"    {key} dtype: {value.dtype} shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    feed_forward_pt = FeedForward_pt(2048, 4 * 2048, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feed_forward_pt.w1.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.gate_proj.weight\"])\n",
    "    feed_forward_pt.w2.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.down_proj.weight\"])\n",
    "    feed_forward_pt.w3.weight.copy_(layer10_weights_pt[\"model.layers.10.mlp.up_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = feed_forward_pt.forward(x_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jax = feed_forward_jax(x_jax,\n",
    "    gate = layer10_weights_jax[\"model.layers.10.mlp.gate_proj.weight\"], \n",
    "    down = layer10_weights_jax[\"model.layers.10.mlp.down_proj.weight\"], \n",
    "    up   = layer10_weights_jax[\"model.layers.10.mlp.up_proj.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.845371e-05"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_torch.detach().numpy() - np.array(y_jax))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
