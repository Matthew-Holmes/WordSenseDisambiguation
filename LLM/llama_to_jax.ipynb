{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama to Jax\n",
    "\n",
    "In this notebook we test the conversion of the Llama architecture to use Jax as the backend, by isolating the components individually and ensuring that they yield the same results, as function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RMSNorm`\n",
    "\n",
    "Here we'll demonstrate how to create jax arrays from the pytorch parameters, and see how our jax implementation of `RMSNorm` compares to the one we already know is a part of the working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded source norm\n",
      "loaded jax norm\n"
     ]
    }
   ],
   "source": [
    "# load the functions to compare\n",
    "from llama.model import RMSNorm as RMSNorm_pt\n",
    "print(\"loaded source norm\")\n",
    "from llama_jax.model import RMSNorm as RMSNorm_jax\n",
    "print(\"loaded jax norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights to find a good demo tensor\n",
    "[key for key in weights.keys() if 'norm' in key][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-04 14:33:29,574:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# pytorch tensor --> jax array\n",
    "rms_weights_pt = weights['model.layers.0.input_layernorm.weight']\n",
    "rms_weights_jax = jnp.array(rms_weights_pt.detach().float().numpy(), dtype=jnp.bfloat16)\n",
    "\n",
    "print(f\"pytorch: {rms_weights_jax}, jax: {rms_weights_jax}\")\n",
    "print(f\"pytorch shape: {rms_weights_pt.shape}, jax shape: {rms_weights_jax.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the weights in an isolated pytorch RMSNorm module\n",
    "rms_norm_pt = RMSNorm_pt(2048)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # overwrite the RMSNorm weight with the one from the loaded state_dict\n",
    "    rms_norm_pt.weight.copy_(rms_weights_pt)\n",
    "\n",
    "rms_norm_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch output: tensor([[-0.1956, -0.0468, -0.1102,  ..., -0.2386,  0.2464, -0.1231],\n",
      "        [ 0.0766, -0.4311, -0.1458,  ..., -0.3946, -0.2244,  0.1960]])\n"
     ]
    }
   ],
   "source": [
    "# run the pytorch version on a sample tensor to get a \"true\" value\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_torch = torch.randn(2, 2048) # add a batch dim\n",
    "\n",
    "    y_torch = rms_norm_pt(x_torch)\n",
    "    print(\"PyTorch output:\", y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax output: [[-0.19556196 -0.04684718 -0.11015126 ... -0.23862486  0.24641658\n",
      "  -0.12314538]\n",
      " [ 0.0766468  -0.43110186 -0.14580318 ... -0.39460558 -0.22443385\n",
      "   0.19601265]]\n"
     ]
    }
   ],
   "source": [
    "# call our jax implementation and check the output is the same\n",
    "\n",
    "rms_norm_jax = lambda x : RMSNorm_jax(x, rms_weights_jax)\n",
    "\n",
    "x_jax = jnp.array(x_torch.detach().numpy())\n",
    "y_jax = rms_norm_jax(x_jax)\n",
    "\n",
    "print(\"Jax output:\", y_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all looks good :), the pytorch outputs `bfloat16` too, whereas jax has higher precision. We'll worry about that later, since I don't know whether that is the correct behaviour when the function is just one step in the overall architecture (for the pytorch implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
