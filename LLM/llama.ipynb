{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Llama locally\n",
    "\n",
    "In this notebook we load the Llama3.2 1 Billion parameter model, and perform text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/.llama/checkpoints/Llama3.2-1B'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/matt/.llama/checkpoints/Llama3.2-1B')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # It would be great to run this, but it OOMs \n",
    "\n",
    "    from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM\n",
    "    import torch\n",
    "\n",
    "    model_path = 'Llama3.2-1B'\n",
    "    #https://stackoverflow.com/a/78911943\n",
    "\n",
    "    # Load the tokenizer directly from the model path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"tokenizer loaded\")\n",
    "\n",
    "    # Load model configuration from params.json\n",
    "    config = LlamaConfig.from_json_file(f'{model_path}/params.json')\n",
    "    print(\"config loaded\")\n",
    "\n",
    "    # load the model with the specific configs. \n",
    "    model = LlamaForCausalLM(config=config)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    # Load the weights of the model\n",
    "    state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"weights loaded\")\n",
    "\n",
    "    model.eval()\n",
    "    print(\"eval called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from the default file download format\n",
    "\n",
    "When downloaded from llama.com, the files look like this\n",
    "\n",
    "```\n",
    "checklist.chk  config.json  consolidated.00.pth  params.json  tokenizer.model\n",
    "```\n",
    "\n",
    "We want them in the HuggingFace format, to do that I ran this script from the `transformers` package (included here for convenience)\n",
    "\n",
    "```bash\n",
    "python3 convert_llama_to_hf.py --input_dir /home/matt/.llama/checkpoints/Llama3.2-1B --model_size 1B --output_dir /home/matt/.llama/checkpoints/Llama3.2-1B-hf --llama_version 3.2\n",
    "```\n",
    "\n",
    "That then populates the output directory with the desired files, which look like:\n",
    "\n",
    "```\n",
    "config.json  generation_config.json  model.safetensors  special_tokens_map.json  tokenizer.json  tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "# load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # automatically uses float16/bfloat16 if available\n",
    "    low_cpu_mem_usage=True,  # prevents high RAM usage\n",
    "    device_map=\"auto\"  # automatically assigns layers to GPU/CPU based on available memory\n",
    ")\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you? i am ron. i work in a hotel. i want to travel. i am from kenya.\n",
      "Hi I'm Raph, 20 and I live in Scotland. I was in school but have stopped to do my life now. I've been in Germany for a year now and I love it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_text = \"hello how are you?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():  # reduces memory usage\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,  \n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id \n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
