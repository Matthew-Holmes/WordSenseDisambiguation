{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# we want to import some llama source later\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/.llama/checkpoints/Llama3.2-1B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints/Llama3.2-1B')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "break # It would be great to run this, but it OOMs with my meagre 8Gb of RAM\n",
    "\n",
    "model_path = 'Llama3.2-1B'\n",
    "#https://stackoverflow.com/a/78911943\n",
    "\n",
    "# Load the tokenizer directly from the model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# Load model configuration from params.json\n",
    "config = LlamaConfig.from_json_file(f'{model_path}/params.json')\n",
    "print(\"config loaded\")\n",
    "\n",
    "# load the model with the specific configs. \n",
    "model = LlamaForCausalLM(config=config)\n",
    "print(\"model loaded\")\n",
    "\n",
    "# Load the weights of the model\n",
    "state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"weights loaded\")\n",
    "\n",
    "model.eval()\n",
    "print(\"eval called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from the default file download format\n",
    "\n",
    "When downloaded from llama.com, the files look like this\n",
    "\n",
    "```\n",
    "checklist.chk  config.json  consolidated.00.pth  params.json  tokenizer.model\n",
    "```\n",
    "\n",
    "We want them in the HuggingFace format, to do that I ran this script from the `transformers` package (included here for convenience)\n",
    "\n",
    "```bash\n",
    "python3 convert_llama_to_hf.py --input_dir /home/matt/.llama/checkpoints/Llama3.2-1B --model_size 1B --output_dir /home/matt/.llama/checkpoints/Llama3.2-1B-hf --llama_version 3.2\n",
    "```\n",
    "\n",
    "That then populates the output directory with the desired files, which look like:\n",
    "\n",
    "```\n",
    "config.json  generation_config.json  model.safetensors  special_tokens_map.json  tokenizer.json  tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "# load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # automatically uses float16/bfloat16 if available\n",
    "    low_cpu_mem_usage=True,  # prevents high RAM usage\n",
    "    device_map=\"auto\"  # automatica`lly assigns layers to GPU/CPU based on available memory\n",
    ")\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"hello how are you?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():  # reduces memory usage\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,  \n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id \n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into `Transfomer` instance from source code\n",
    "\n",
    "We are interested in the activations through the layers of this model, so it would be good to load create an instance of the `Transformer` object defined in the `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, rotary_embed_len=131072, cache_len=2048)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from llama.model import Transformer, ModelArgs  # Ensure your model classes are imported\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    ffn_dim_multiplier=None,  # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    rotary_embed_len=config.get(\"max_position_embeddings\", 2048),  # map \"max_position_embeddings\"\n",
    "    cache_len = 2048,\n",
    ")\n",
    "\n",
    "print(model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "attention initialising\n",
      " done wq\n",
      " done wk\n",
      " done wv\n",
      " done wo\n",
      " done cache k\n",
      " done cache v\n",
      "Transformer created\n",
      "weights in RAM\n",
      "weights in model\n"
     ]
    }
   ],
   "source": [
    "from llama.model import Transformer\n",
    "\n",
    "# I upgraded to 16Gb of RAM and now this will run - just need to tune the max sequence length as it will preallocate\n",
    "# the caches in the attention blocks based on that value\n",
    "\n",
    "# RAM preservation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16 \n",
    "\n",
    "model = Transformer(model_args)\n",
    "\n",
    "#.to(dtype=torch_dtype)\n",
    "\n",
    "print(\"Transformer created\")\n",
    "\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "print(\"weights in RAM\")\n",
    "\n",
    "model.load_state_dict(weights, strict=False)\n",
    "\n",
    "print(\"weights in model\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): ParallelEmbedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): ColumnParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "        (wk): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wv): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wo): RowParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): ColumnParallelLinear(in_features=2048, out_features=5632, bias=False)\n",
       "        (w2): RowParallelLinear(in_features=5632, out_features=2048, bias=False)\n",
       "        (w3): ColumnParallelLinear(in_features=2048, out_features=5632, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): ColumnParallelLinear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7fc4821f49b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "# go here to find the model file\n",
    "# https://github.com/meta-llama/llama-models/blob/main/models/llama3/api/tokenizer.model\n",
    "\n",
    "new_tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "new_tok = Tokenizer(new_tok_path)\n",
    "new_tok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
