{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Llama locally\n",
    "\n",
    "In this notebook we load the Llama3.2 1 Billion parameter model, and perform text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/.llama/checkpoints/Llama3.2-1B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/matt/.llama/checkpoints/Llama3.2-1B')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # It would be great to run this, but it OOMs \n",
    "\n",
    "    from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM\n",
    "    import torch\n",
    "\n",
    "    model_path = 'Llama3.2-1B'\n",
    "    #https://stackoverflow.com/a/78911943\n",
    "\n",
    "    # Load the tokenizer directly from the model path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"tokenizer loaded\")\n",
    "\n",
    "    # Load model configuration from params.json\n",
    "    config = LlamaConfig.from_json_file(f'{model_path}/params.json')\n",
    "    print(\"config loaded\")\n",
    "\n",
    "    # load the model with the specific configs. \n",
    "    model = LlamaForCausalLM(config=config)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    # Load the weights of the model\n",
    "    state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"weights loaded\")\n",
    "\n",
    "    model.eval()\n",
    "    print(\"eval called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from the default file download format\n",
    "\n",
    "When downloaded from llama.com, the files look like this\n",
    "\n",
    "```\n",
    "checklist.chk  config.json  consolidated.00.pth  params.json  tokenizer.model\n",
    "```\n",
    "\n",
    "We want them in the HuggingFace format, to do that I ran this script from the `transformers` package (included here for convenience)\n",
    "\n",
    "```bash\n",
    "python3 convert_llama_to_hf.py --input_dir /home/matt/.llama/checkpoints/Llama3.2-1B --model_size 1B --output_dir /home/matt/.llama/checkpoints/Llama3.2-1B-hf --llama_version 3.2\n",
    "```\n",
    "\n",
    "That then populates the output directory with the desired files, which look like:\n",
    "\n",
    "```\n",
    "config.json  generation_config.json  model.safetensors  special_tokens_map.json  tokenizer.json  tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "# load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # automatically uses float16/bfloat16 if available\n",
    "    low_cpu_mem_usage=True,  # prevents high RAM usage\n",
    "    device_map=\"auto\"  # automatically assigns layers to GPU/CPU based on available memory\n",
    ")\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is  France  is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_text = \"The capital of France is \"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():  # reduces memory usage\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,  \n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.0\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is 100% French (in terms of culture and traditions), and Paris has something for everybody to see and do. It is a must-see city for anyone wishing to explore France.\n",
      "The city of Paris has a population of 2.14 million (city area), and 20.2 million people (including suburbs and metropolitan area).\n",
      "It has the largest GDP in France.\n",
      "It has the best infrastructure in France, with one of the worldâ€™s busiest airports (Charles de Gaulle).\n",
      "The capital city of France is 100% French (in terms of culture and traditions), and Paris has something for everybody to see and do. It is a must-see city for anyone wishing to explore France.\n",
      "The city of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_text = \"The capital of France is \"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():  # reduces memory usage\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,  \n",
    "        temperature=1.2,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalising repetition\n",
    "\n",
    "This is done via a parameter - see the large difference it makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# check the rope params for when we're doing stuff with the source code\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "# Print configuration details\n",
    "print(config.rope_scaling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000.0\n"
     ]
    }
   ],
   "source": [
    "print(config.rope_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'config', 'loss_type', 'name_or_path', 'warnings_issued', 'generation_config', '_keep_in_fp32_modules', 'vocab_size', '_is_hf_initialized', 'hf_device_map'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'config', 'loss_type', 'name_or_path', 'warnings_issued', 'generation_config', '_keep_in_fp32_modules', 'padding_idx', 'vocab_size', 'gradient_checkpointing', '_tp_plan', '_pp_plan', '_is_hf_initialized'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model'].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.6360e-01, 4.4037e-01, 2.9223e-01, 1.9392e-01, 1.2869e-01,\n",
       "        8.5397e-02, 5.6670e-02, 3.7606e-02, 2.4955e-02, 1.6560e-02, 1.0990e-02,\n",
       "        7.2927e-03, 4.8394e-03, 3.2114e-03, 1.2905e-03, 4.2956e-04, 9.7083e-05,\n",
       "        1.9462e-05, 1.2915e-05, 8.5703e-06, 5.6872e-06, 3.7741e-06, 2.5045e-06,\n",
       "        1.6620e-06, 1.1029e-06, 7.3187e-07, 4.8567e-07, 3.2229e-07, 2.1387e-07,\n",
       "        1.4193e-07, 9.4183e-08])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model']._modules['rotary_emb'].original_inv_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'rope_type', 'max_seq_len_cached', 'original_max_seq_len', 'config', 'rope_init_fn', 'attention_scaling', 'original_inv_freq', '_is_hf_initialized'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model']._modules['rotary_emb'].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model']._modules['rotary_emb'].max_seq_len_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model']._modules['rotary_emb'].original_max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model']._modules['rotary_emb'].attention_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
