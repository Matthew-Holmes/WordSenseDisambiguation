{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/.llama/checkpoints/Llama3.2-1B'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/matt/.llama/checkpoints/Llama3.2-1B')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (2047338042.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[59], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "# It would be great to run this, but it OOMs with my meagre 8Gb of RAM\n",
    "from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "break\n",
    "\n",
    "model_path = 'Llama3.2-1B'\n",
    "#https://stackoverflow.com/a/78911943\n",
    "\n",
    "# Load the tokenizer directly from the model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# Load model configuration from params.json\n",
    "config = LlamaConfig.from_json_file(f'{model_path}/params.json')\n",
    "print(\"config loaded\")\n",
    "\n",
    "# load the model with the specific configs. \n",
    "model = LlamaForCausalLM(config=config)\n",
    "print(\"model loaded\")\n",
    "\n",
    "# Load the weights of the model\n",
    "state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"weights loaded\")\n",
    "\n",
    "model.eval()\n",
    "print(\"eval called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from the default file download format\n",
    "\n",
    "When downloaded from llama.com, the files look like this\n",
    "\n",
    "```\n",
    "checklist.chk  config.json  consolidated.00.pth  params.json  tokenizer.model\n",
    "```\n",
    "\n",
    "We want them in the HuggingFace format, to do that I ran this script from the `transformers` package (included here for convenience)\n",
    "\n",
    "```bash\n",
    "python3 convert_llama_to_hf.py --input_dir /home/matt/.llama/checkpoints/Llama3.2-1B --model_size 1B --output_dir /home/matt/.llama/checkpoints/Llama3.2-1B-hf --llama_version 3.2\n",
    "```\n",
    "\n",
    "That then populates the output directory with the desired files, which look like:\n",
    "\n",
    "```\n",
    "config.json  generation_config.json  model.safetensors  special_tokens_map.json  tokenizer.json  tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # automatically uses float16/bfloat16 if available\n",
    "    low_cpu_mem_usage=True,  # prevents high RAM usage\n",
    "    device_map=\"auto\"  # automatically assigns layers to GPU/CPU based on available memory\n",
    ")\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you? how are you all doing??\n",
      "we’re here with a fun post….\n",
      "a little something to make your day a little better.\n",
      "i’ve seen several bloggers use this on their blogs, and it’s so simple, so clean…just a way to help out, so that you know you’re doing the right thing.\n",
      "so here’s what you do: you write your favorite blogger a simple, sweet message of support, and they write you a simple, sweet message in return.\n",
      "that’s it. you don’t have to do anything special or hard to make the person happy…just a simple, sweet post, to let them know you care.\n",
      "it’s a great way to let someone know they have your support,\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_text = \"how are you?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():  # reduces memory usage\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,  \n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id \n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
