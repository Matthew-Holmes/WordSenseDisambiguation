{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# we want to import some llama source later\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matt/.llama/checkpoints/Llama3.2-1B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints/Llama3.2-1B')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (1351421061.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    break # It would be great to run this, but it OOMs with my meagre 8Gb of RAM\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "break # It would be great to run this, but it OOMs with my meagre 8Gb of RAM\n",
    "\n",
    "model_path = 'Llama3.2-1B'\n",
    "#https://stackoverflow.com/a/78911943\n",
    "\n",
    "# Load the tokenizer directly from the model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# Load model configuration from params.json\n",
    "config = LlamaConfig.from_json_file(f'{model_path}/params.json')\n",
    "print(\"config loaded\")\n",
    "\n",
    "# load the model with the specific configs. \n",
    "model = LlamaForCausalLM(config=config)\n",
    "print(\"model loaded\")\n",
    "\n",
    "# Load the weights of the model\n",
    "state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"weights loaded\")\n",
    "\n",
    "model.eval()\n",
    "print(\"eval called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from the default file download format\n",
    "\n",
    "When downloaded from llama.com, the files look like this\n",
    "\n",
    "```\n",
    "checklist.chk  config.json  consolidated.00.pth  params.json  tokenizer.model\n",
    "```\n",
    "\n",
    "We want them in the HuggingFace format, to do that I ran this script from the `transformers` package (included here for convenience)\n",
    "\n",
    "```bash\n",
    "python3 convert_llama_to_hf.py --input_dir /home/matt/.llama/checkpoints/Llama3.2-1B --model_size 1B --output_dir /home/matt/.llama/checkpoints/Llama3.2-1B-hf --llama_version 3.2\n",
    "```\n",
    "\n",
    "That then populates the output directory with the desired files, which look like:\n",
    "\n",
    "```\n",
    "config.json  generation_config.json  model.safetensors  special_tokens_map.json  tokenizer.json  tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_path = \"Llama3.2-1B-hf\"\n",
    "\n",
    "# load model with reduced precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",  # automatically uses float16/bfloat16 if available\n",
    "    low_cpu_mem_usage=True,  # prevents high RAM usage\n",
    "    device_map=\"auto\"  # automatica`lly assigns layers to GPU/CPU based on available memory\n",
    ")\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"hello how are you?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():  # reduces memory usage\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=150,  \n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id \n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into `Transfomer` instance from source code\n",
    "\n",
    "We are interested in the activations through the layers of this model, so it would be good to load create an instance of the `Transformer` object defined in the `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.4545454545454546, norm_eps=1e-05, rope_theta=500000, use_scaled_rope=False, max_batch_size=32, rotary_embed_len=131072, cache_len=2048, vision_chunk_size=-1, vision_max_num_chunks=4, vision_num_cross_attention_layers=-1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from llama.model_new import Transformer, ModelArgs  # Ensure your model classes are imported\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    rotary_embed_len=config.get(\"max_position_embeddings\", 2048),  # map \"max_position_embeddings\"\n",
    "    cache_len = 2048,\n",
    "    ffn_dim_multiplier = 16 / 11\n",
    ")\n",
    "\n",
    "print(model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "Transformer created\n",
      "weights in RAM\n"
     ]
    }
   ],
   "source": [
    "from llama.model_new import Transformer\n",
    "\n",
    "# I upgraded to 16Gb of RAM and now this will run - just need to tune the max sequence length as it will preallocate\n",
    "# the caches in the attention blocks based on that value\n",
    "\n",
    "# RAM preservation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16 \n",
    "\n",
    "model = Transformer(model_args)\n",
    "\n",
    "#.to(dtype=torch_dtype)\n",
    "\n",
    "print(\"Transformer created\")\n",
    "\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "print(\"weights in RAM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights' names don't match what is in the code I found\n",
    "# so we rename them\n",
    "\n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "# create a new state dict with corrected names\n",
    "fixed_state_dict = {}\n",
    "\n",
    "for key in weights.keys():\n",
    "    \n",
    "    new_key = key\n",
    "    new_key = new_key.replace(\"model.\", \"\")\n",
    "    \n",
    "    new_key = new_key.replace(\"embed_tokens.weight\", \"tok_embeddings.weight\")\n",
    "\n",
    "    new_key = new_key.replace(\"self_attn.q_proj\", \"attention.wq\")\n",
    "    new_key = new_key.replace(\"self_attn.k_proj\", \"attention.wk\")\n",
    "    new_key = new_key.replace(\"self_attn.v_proj\", \"attention.wv\")\n",
    "    new_key = new_key.replace(\"self_attn.o_proj\", \"attention.wo\")\n",
    "\n",
    "    new_key = new_key.replace(\"mlp.gate_proj\", \"feed_forward.w1\")\n",
    "    new_key = new_key.replace(\"mlp.up_proj\", \"feed_forward.w3\")\n",
    "    new_key = new_key.replace(\"mlp.down_proj\", \"feed_forward.w2\")\n",
    "\n",
    "    new_key = new_key.replace(\"input_layernorm\", \"attention_norm\")\n",
    "    new_key = new_key.replace(\"post_attention_layernorm\", \"ffn_norm\")\n",
    "\n",
    "    new_key = new_key.replace(\"model.norm\", \"norm\")\n",
    "\n",
    "    fixed_state_dict[new_key] = weights[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'layers.0.attention_norm.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.ffn_norm.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wo.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wv.weight', 'layers.1.attention_norm.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.ffn_norm.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wo.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wv.weight', 'layers.10.attention_norm.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.ffn_norm.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wo.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wv.weight', 'layers.11.attention_norm.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.ffn_norm.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wo.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wv.weight', 'layers.12.attention_norm.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.ffn_norm.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wo.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wv.weight', 'layers.13.attention_norm.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.ffn_norm.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wo.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wv.weight', 'layers.14.attention_norm.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.ffn_norm.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wo.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wv.weight', 'layers.15.attention_norm.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.ffn_norm.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wo.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wv.weight', 'layers.2.attention_norm.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.ffn_norm.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wo.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wv.weight', 'layers.3.attention_norm.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.ffn_norm.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wo.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wv.weight', 'layers.4.attention_norm.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.ffn_norm.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wo.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wv.weight', 'layers.5.attention_norm.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.ffn_norm.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wo.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wv.weight', 'layers.6.attention_norm.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.ffn_norm.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wo.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wv.weight', 'layers.7.attention_norm.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.ffn_norm.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wo.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wv.weight', 'layers.8.attention_norm.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.ffn_norm.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wo.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wv.weight', 'layers.9.attention_norm.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.ffn_norm.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wo.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wv.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with corrected state_dict\n"
     ]
    }
   ],
   "source": [
    "# Load the corrected state dict\n",
    "model.load_state_dict(fixed_state_dict, strict = False) # since the output weights are tied, these are already correct and not loaded - but the Transformer class expects the; so load non-strictly\n",
    "\n",
    "print(\"loaded model with corrected state_dict\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): ColumnParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "        (wk): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wv): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wo): RowParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): RowParallelLinear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): ColumnParallelLinear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tie the output embedding manually\n",
    "\n",
    "model.output.weight.data = model.tok_embeddings.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7f39a6d08c20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "# go here to find the model file\n",
    "# https://github.com/meta-llama/llama-models/blob/main/models/llama3/api/tokenizer.model\n",
    "\n",
    "new_tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "new_tok = Tokenizer(new_tok_path)\n",
    "new_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation_new import Llama\n",
    "\n",
    "llama = Llama(model, new_tok, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.generation_new.Llama at 0x7f39a6d09190>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInput to model:\n",
      "<|begin_of_text|>The capital of France is\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama.generation_new import CompletionPrediction\n",
    "\n",
    "# A high top_p seems necessary here, otherwise we just get strings of numbers\n",
    "res: CompletionPrediction = llama.text_completion(\"The capital of France is\", max_gen_len=50, top_p = 0.99, temperature = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris, the capital of France. It is the largest city of France. It is the capital of France. It is the largest city of France. It is the capital of France. It is the capital of France. It is the capital of France'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.0769612789154053],\n",
       " [-1.274505853652954],\n",
       " [-1.5088263750076294],\n",
       " [-0.7424589395523071],\n",
       " [-0.27164074778556824],\n",
       " [-0.2794206738471985],\n",
       " [-1.534311294555664],\n",
       " [-1.5697606801986694],\n",
       " [-0.44362616539001465],\n",
       " [-1.2413744926452637],\n",
       " [-2.3812146186828613],\n",
       " [-0.43809354305267334],\n",
       " [-0.9235026836395264],\n",
       " [-0.547737181186676],\n",
       " [-0.9087198376655579],\n",
       " [-1.3606833219528198],\n",
       " [-0.4652999937534332],\n",
       " [-0.7953785061836243],\n",
       " [-0.6358326077461243],\n",
       " [-0.19882136583328247],\n",
       " [-0.17556649446487427],\n",
       " [-0.364422082901001],\n",
       " [-0.3728608191013336],\n",
       " [-0.08018721640110016],\n",
       " [-0.10402780026197433],\n",
       " [-0.2573114037513733],\n",
       " [-0.08796743303537369],\n",
       " [-0.537945568561554],\n",
       " [-0.08411218225955963],\n",
       " [-0.33591169118881226],\n",
       " [-0.20135679841041565],\n",
       " [-0.04040507599711418],\n",
       " [-0.04492664337158203],\n",
       " [-0.27832546830177307],\n",
       " [-0.06479234248399734],\n",
       " [-0.07717431336641312],\n",
       " [-0.6966521143913269],\n",
       " [-0.3307095766067505],\n",
       " [-0.08723539859056473],\n",
       " [-0.07333430647850037],\n",
       " [-0.4753025770187378],\n",
       " [-0.12660081684589386],\n",
       " [-0.13861623406410217],\n",
       " [-0.3909229338169098],\n",
       " [-0.3859510123729706],\n",
       " [-0.11233396828174591],\n",
       " [-0.1978680044412613],\n",
       " [-0.3734161853790283],\n",
       " [-0.15689346194267273],\n",
       " [-0.2692156732082367]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.logprobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
