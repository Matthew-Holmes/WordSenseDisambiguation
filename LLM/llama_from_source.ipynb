{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Llama from source code\n",
    "\n",
    "In this notebook we use the Pytorch source code to run the model locally, the goal is to pare back the extra stuff included so the code can be translated to use Jax\n",
    "\n",
    "## Paring back\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "I'll leave this as is, since it is cheap, and can be injected to the final LLama instance\n",
    "\n",
    "### Model\n",
    "\n",
    "There are some references to image parameters that I removed from this.\n",
    "\n",
    "The remaining complexity relates to the resizing functions - since I'm only interested in this model (for now) then I can remove that and refactor the model args to remove the artificial 16/11 scale factor I had to introduce to get this to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# we want to import some llama source later\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into `Transfomer` instance from source code\n",
    "\n",
    "We are interested in the activations through the layers of this model, so it would be good to load create an instance of the `Transformer` object defined in the `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, norm_eps=1e-05, rope_theta=500000, use_scaled_rope=True, rope_scale_factor=32.0, max_batch_size=32, original_rotary_embed_len=8192, cache_len=2048)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from llama.model import ModelArgs\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    use_scale_rope=True, # this is how it was in llama.ipynb\n",
    "    rope_scale_factor=config.get(\"rope_scaling\").get(\"factor\"),\n",
    "    original_rotary_embed_len=config.get(\"rope_scaling\").get(\"original_max_position_embeddings\"),\n",
    "    cache_len = 2048,\n",
    ")\n",
    "\n",
    "print(model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'attention_dropout': 0.0, 'bos_token_id': 128000, 'eos_token_id': 128001, 'head_dim': 64, 'hidden_act': 'silu', 'hidden_size': 2048, 'initializer_range': 0.02, 'intermediate_size': 8192, 'max_position_embeddings': 131072, 'mlp_bias': False, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 16, 'num_key_value_heads': 8, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'rope_theta': 500000.0, 'tie_word_embeddings': True, 'torch_dtype': 'bfloat16', 'transformers_version': '4.49.0', 'use_cache': True, 'vocab_size': 128256}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "orinal freqs: tensor([1.0000e+00, 6.6360e-01, 4.4037e-01, 2.9223e-01, 1.9392e-01, 1.2869e-01,\n",
      "        8.5397e-02, 5.6670e-02, 3.7606e-02, 2.4955e-02, 1.6560e-02, 1.0990e-02,\n",
      "        7.2927e-03, 4.8394e-03, 3.2114e-03, 2.1311e-03, 1.4142e-03, 9.3847e-04,\n",
      "        6.2277e-04, 4.1327e-04, 2.7425e-04, 1.8199e-04, 1.2077e-04, 8.0143e-05,\n",
      "        5.3183e-05, 3.5292e-05, 2.3420e-05, 1.5542e-05, 1.0313e-05, 6.8440e-06,\n",
      "        4.5417e-06, 3.0139e-06])\n",
      "rescaled freqs: tensor([1.0000e+00, 6.6360e-01, 4.4037e-01, 2.9223e-01, 1.9392e-01, 1.2869e-01,\n",
      "        8.5397e-02, 5.6670e-02, 3.7606e-02, 2.4955e-02, 1.6560e-02, 1.0990e-02,\n",
      "        7.2927e-03, 4.8394e-03, 3.2114e-03, 2.1311e-03, 1.4142e-03, 9.3847e-04,\n",
      "        1.9462e-05, 1.2915e-05, 8.5703e-06, 5.6872e-06, 3.7741e-06, 2.5045e-06,\n",
      "        1.6620e-06, 1.1029e-06, 7.3187e-07, 4.8567e-07, 3.2229e-07, 2.1387e-07,\n",
      "        1.4193e-07, 9.4183e-08])\n",
      "Transformer created\n",
      "weights in RAM\n"
     ]
    }
   ],
   "source": [
    "from llama.model import Transformer\n",
    "\n",
    "# I upgraded to 16Gb of RAM and now this will run - just need to tune the max sequence length as it will preallocate\n",
    "# the caches in the attention blocks based on that value\n",
    "\n",
    "# RAM preservation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16 \n",
    "\n",
    "model = Transformer(model_args)\n",
    "\n",
    "#.to(dtype=torch_dtype)\n",
    "\n",
    "print(\"Transformer created\")\n",
    "\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "print(\"weights in RAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the base frequencies for RoPE are the same as what we had in the llama.ipynb notebook; after we apply the scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights' names don't match what is in the code I found\n",
    "# so we rename them\n",
    "\n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "# create a new state dict with corrected names\n",
    "fixed_state_dict = {}\n",
    "\n",
    "for key in weights.keys():\n",
    "    \n",
    "    new_key = key\n",
    "    new_key = new_key.replace(\"model.\", \"\")\n",
    "    \n",
    "    new_key = new_key.replace(\"embed_tokens.weight\", \"tok_embeddings.weight\")\n",
    "\n",
    "    new_key = new_key.replace(\"self_attn.q_proj\", \"attention.wq\")\n",
    "    new_key = new_key.replace(\"self_attn.k_proj\", \"attention.wk\")\n",
    "    new_key = new_key.replace(\"self_attn.v_proj\", \"attention.wv\")\n",
    "    new_key = new_key.replace(\"self_attn.o_proj\", \"attention.wo\")\n",
    "\n",
    "    new_key = new_key.replace(\"mlp.gate_proj\", \"feed_forward.w1\")\n",
    "    new_key = new_key.replace(\"mlp.up_proj\", \"feed_forward.w3\")\n",
    "    new_key = new_key.replace(\"mlp.down_proj\", \"feed_forward.w2\")\n",
    "\n",
    "    new_key = new_key.replace(\"input_layernorm\", \"attention_norm\")\n",
    "    new_key = new_key.replace(\"post_attention_layernorm\", \"ffn_norm\")\n",
    "\n",
    "    new_key = new_key.replace(\"model.norm\", \"norm\")\n",
    "\n",
    "    fixed_state_dict[new_key] = weights[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'layers.0.attention_norm.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.ffn_norm.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wo.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wv.weight', 'layers.1.attention_norm.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.ffn_norm.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wo.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wv.weight', 'layers.10.attention_norm.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.ffn_norm.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wo.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wv.weight', 'layers.11.attention_norm.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.ffn_norm.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wo.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wv.weight', 'layers.12.attention_norm.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.ffn_norm.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wo.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wv.weight', 'layers.13.attention_norm.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.ffn_norm.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wo.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wv.weight', 'layers.14.attention_norm.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.ffn_norm.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wo.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wv.weight', 'layers.15.attention_norm.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.ffn_norm.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wo.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wv.weight', 'layers.2.attention_norm.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.ffn_norm.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wo.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wv.weight', 'layers.3.attention_norm.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.ffn_norm.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wo.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wv.weight', 'layers.4.attention_norm.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.ffn_norm.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wo.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wv.weight', 'layers.5.attention_norm.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.ffn_norm.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wo.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wv.weight', 'layers.6.attention_norm.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.ffn_norm.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wo.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wv.weight', 'layers.7.attention_norm.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.ffn_norm.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wo.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wv.weight', 'layers.8.attention_norm.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.ffn_norm.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wo.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wv.weight', 'layers.9.attention_norm.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.ffn_norm.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wo.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wv.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with corrected state_dict\n"
     ]
    }
   ],
   "source": [
    "# Load the corrected state dict\n",
    "model.load_state_dict(fixed_state_dict, strict = False) # since the output weights are tied, these are already correct and not loaded - but the Transformer class expects the; so load non-strictly\n",
    "\n",
    "print(\"loaded model with corrected state_dict\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): ColumnParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "        (wk): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wv): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wo): RowParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): RowParallelLinear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): ColumnParallelLinear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tie the output embedding manually\n",
    "\n",
    "model.output.weight.data = model.tok_embeddings.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7f1fcc9984a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "# go here to find the model file\n",
    "# https://github.com/meta-llama/llama-models/blob/main/models/llama3/api/tokenizer.model (689c7f2)\n",
    "\n",
    "new_tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "new_tok = Tokenizer(new_tok_path)\n",
    "new_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama\n",
    "\n",
    "llama = Llama(model, new_tok, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.generation.Llama at 0x7f1f91030f20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInput to model:\n",
      "Hello how are you?\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama.generation import CompletionPrediction\n",
    "\n",
    "# A high top_p seems necessary here, otherwise we just get strings of numbers\n",
    "res: CompletionPrediction = llama.text_completion(\"Hello how are you?\", max_gen_len=50, top_p = 0.9, temperature = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have to tell you that’s all right now let me ask and we’re going on the next thing is it looks like there might be a little bit of. Okay so he was not gonna do this for her just in terms if she does that'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.962317943572998],\n",
       " [-3.5501389503479004],\n",
       " [-2.9875898361206055],\n",
       " [-2.7195866107940674],\n",
       " [-0.28186988830566406],\n",
       " [-1.502641201019287],\n",
       " [-3.476128339767456],\n",
       " [-2.60990047454834],\n",
       " [-1.342206597328186],\n",
       " [-1.5903857946395874],\n",
       " [-4.463281154632568],\n",
       " [-1.6123003959655762],\n",
       " [-2.503183364868164],\n",
       " [-5.322690010070801],\n",
       " [-3.641578197479248],\n",
       " [-1.684828758239746],\n",
       " [-2.94458270072937],\n",
       " [-3.2123425006866455],\n",
       " [-2.4785866737365723],\n",
       " [-3.881150007247925],\n",
       " [-2.8626809120178223],\n",
       " [-2.163466453552246],\n",
       " [-3.1696324348449707],\n",
       " [-5.1420159339904785],\n",
       " [-0.49252355098724365],\n",
       " [-3.9356002807617188],\n",
       " [-4.741507530212402],\n",
       " [-0.4936402440071106],\n",
       " [-1.9052330255508423],\n",
       " [-2.4992289543151855],\n",
       " [-0.7705128192901611],\n",
       " [-1.187638282775879],\n",
       " [-6.011882305145264],\n",
       " [-4.2115960121154785],\n",
       " [-1.6770695447921753],\n",
       " [-5.363551139831543],\n",
       " [-2.9623208045959473],\n",
       " [-4.679755210876465],\n",
       " [-4.071743965148926],\n",
       " [-3.1328444480895996],\n",
       " [-3.064316749572754],\n",
       " [-4.490031719207764],\n",
       " [-5.458767890930176],\n",
       " [-5.039636611938477],\n",
       " [-4.742769718170166],\n",
       " [-6.23613166809082],\n",
       " [-5.184830665588379],\n",
       " [-3.3390650749206543],\n",
       " [-4.540905952453613],\n",
       " [-3.0623714923858643]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.logprobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
