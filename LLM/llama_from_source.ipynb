{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Llama from source code\n",
    "\n",
    "In this notebook we use the Pytorch source code to run the model locally, the goal is to pare back the extra stuff included so the code can be translated to use Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# we want to import some llama source later\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into `Transfomer` instance from source code\n",
    "\n",
    "We are interested in the activations through the layers of this model, so it would be good to load create an instance of the `Transformer` object defined in the `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.4545454545454546, norm_eps=1e-05, rope_theta=500000, use_scaled_rope=False, max_batch_size=32, rotary_embed_len=131072, cache_len=2048, vision_chunk_size=-1, vision_max_num_chunks=4, vision_num_cross_attention_layers=-1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from llama.model_new import ModelArgs\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    rotary_embed_len=config.get(\"max_position_embeddings\", 2048),  # map \"max_position_embeddings\"\n",
    "    cache_len = 2048,\n",
    "    ffn_dim_multiplier = 16 / 11\n",
    ")\n",
    "\n",
    "print(model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "Transformer created\n",
      "weights in RAM\n"
     ]
    }
   ],
   "source": [
    "from llama.model_new import Transformer\n",
    "\n",
    "# I upgraded to 16Gb of RAM and now this will run - just need to tune the max sequence length as it will preallocate\n",
    "# the caches in the attention blocks based on that value\n",
    "\n",
    "# RAM preservation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16 \n",
    "\n",
    "model = Transformer(model_args)\n",
    "\n",
    "#.to(dtype=torch_dtype)\n",
    "\n",
    "print(\"Transformer created\")\n",
    "\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "print(\"weights in RAM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights' names don't match what is in the code I found\n",
    "# so we rename them\n",
    "\n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "# create a new state dict with corrected names\n",
    "fixed_state_dict = {}\n",
    "\n",
    "for key in weights.keys():\n",
    "    \n",
    "    new_key = key\n",
    "    new_key = new_key.replace(\"model.\", \"\")\n",
    "    \n",
    "    new_key = new_key.replace(\"embed_tokens.weight\", \"tok_embeddings.weight\")\n",
    "\n",
    "    new_key = new_key.replace(\"self_attn.q_proj\", \"attention.wq\")\n",
    "    new_key = new_key.replace(\"self_attn.k_proj\", \"attention.wk\")\n",
    "    new_key = new_key.replace(\"self_attn.v_proj\", \"attention.wv\")\n",
    "    new_key = new_key.replace(\"self_attn.o_proj\", \"attention.wo\")\n",
    "\n",
    "    new_key = new_key.replace(\"mlp.gate_proj\", \"feed_forward.w1\")\n",
    "    new_key = new_key.replace(\"mlp.up_proj\", \"feed_forward.w3\")\n",
    "    new_key = new_key.replace(\"mlp.down_proj\", \"feed_forward.w2\")\n",
    "\n",
    "    new_key = new_key.replace(\"input_layernorm\", \"attention_norm\")\n",
    "    new_key = new_key.replace(\"post_attention_layernorm\", \"ffn_norm\")\n",
    "\n",
    "    new_key = new_key.replace(\"model.norm\", \"norm\")\n",
    "\n",
    "    fixed_state_dict[new_key] = weights[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'layers.0.attention_norm.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.ffn_norm.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wo.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wv.weight', 'layers.1.attention_norm.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.ffn_norm.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wo.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wv.weight', 'layers.10.attention_norm.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.ffn_norm.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wo.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wv.weight', 'layers.11.attention_norm.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.ffn_norm.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wo.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wv.weight', 'layers.12.attention_norm.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.ffn_norm.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wo.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wv.weight', 'layers.13.attention_norm.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.ffn_norm.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wo.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wv.weight', 'layers.14.attention_norm.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.ffn_norm.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wo.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wv.weight', 'layers.15.attention_norm.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.ffn_norm.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wo.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wv.weight', 'layers.2.attention_norm.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.ffn_norm.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wo.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wv.weight', 'layers.3.attention_norm.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.ffn_norm.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wo.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wv.weight', 'layers.4.attention_norm.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.ffn_norm.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wo.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wv.weight', 'layers.5.attention_norm.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.ffn_norm.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wo.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wv.weight', 'layers.6.attention_norm.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.ffn_norm.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wo.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wv.weight', 'layers.7.attention_norm.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.ffn_norm.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wo.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wv.weight', 'layers.8.attention_norm.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.ffn_norm.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wo.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wv.weight', 'layers.9.attention_norm.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.ffn_norm.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wo.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wv.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with corrected state_dict\n"
     ]
    }
   ],
   "source": [
    "# Load the corrected state dict\n",
    "model.load_state_dict(fixed_state_dict, strict = False) # since the output weights are tied, these are already correct and not loaded - but the Transformer class expects the; so load non-strictly\n",
    "\n",
    "print(\"loaded model with corrected state_dict\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): ColumnParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "        (wk): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wv): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wo): RowParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): RowParallelLinear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): ColumnParallelLinear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tie the output embedding manually\n",
    "\n",
    "model.output.weight.data = model.tok_embeddings.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7f4282dd4890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "# go here to find the model file\n",
    "# https://github.com/meta-llama/llama-models/blob/main/models/llama3/api/tokenizer.model (689c7f2)\n",
    "\n",
    "new_tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "new_tok = Tokenizer(new_tok_path)\n",
    "new_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation_new import Llama\n",
    "\n",
    "llama = Llama(model, new_tok, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.generation_new.Llama at 0x7f4282d380e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInput to model:\n",
      "<|begin_of_text|>The capital of France is\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama.generation_new import CompletionPrediction\n",
    "\n",
    "# A high top_p seems necessary here, otherwise we just get strings of numbers\n",
    "res: CompletionPrediction = llama.text_completion(\"The capital of France is\", max_gen_len=50, top_p = 0.99, temperature = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.0232808589935303],\n",
       " [-1.8558874130249023],\n",
       " [-0.35268905758857727],\n",
       " [-0.4167436957359314],\n",
       " [-1.016157627105713],\n",
       " [-0.8968588709831238],\n",
       " [-1.2876267433166504],\n",
       " [-1.806140661239624],\n",
       " [-1.2675158977508545],\n",
       " [-0.26237577199935913],\n",
       " [-0.19873137772083282],\n",
       " [-0.38704994320869446],\n",
       " [-0.7489703893661499],\n",
       " [-0.43344929814338684],\n",
       " [-0.883790910243988],\n",
       " [-0.17068198323249817],\n",
       " [-0.1169423907995224],\n",
       " [-0.1793278455734253],\n",
       " [-0.1501794308423996],\n",
       " [-0.3163699507713318],\n",
       " [-0.3283524513244629],\n",
       " [-0.5362377166748047],\n",
       " [-0.11693623661994934],\n",
       " [-0.08320949971675873],\n",
       " [-0.18952108919620514],\n",
       " [-0.1585375815629959],\n",
       " [-0.4404500126838684],\n",
       " [-0.2826058566570282],\n",
       " [-0.28091034293174744],\n",
       " [-0.06224074587225914],\n",
       " [-0.0804356187582016],\n",
       " [-0.15348391234874725],\n",
       " [-0.2140636295080185],\n",
       " [-0.5775657892227173],\n",
       " [-0.24491234123706818],\n",
       " [-0.27858924865722656],\n",
       " [-0.15325607359409332],\n",
       " [-0.18884742259979248],\n",
       " [-0.1465480774641037],\n",
       " [-0.5145536065101624],\n",
       " [-1.1685043573379517],\n",
       " [-0.27316537499427795],\n",
       " [-0.2421436309814453],\n",
       " [-0.3215765655040741],\n",
       " [-0.42329928278923035],\n",
       " [-0.1940583437681198],\n",
       " [-0.3852470815181732],\n",
       " [-0.38513022661209106],\n",
       " [-0.27412375807762146],\n",
       " [-0.3698230981826782]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
