{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Llama from source code\n",
    "\n",
    "In this notebook we use the Pytorch source code to run the model locally, the goal is to pare back the extra stuff included so the code can be translated to use Jax\n",
    "\n",
    "## Paring back\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "I'll leave this as is, since it is cheap, and can be injected to the final LLama instance\n",
    "\n",
    "### Model\n",
    "\n",
    "There are some references to image parameters that I removed from this.\n",
    "\n",
    "The remaining complexity relates to the resizing functions - since I'm only interested in this model (for now) then I can remove that and refactor the model args to remove the artificial 16/11 scale factor I had to introduce to get this to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# we want to import some llama source later\n",
    "os.getcwd()\n",
    "project_path = os.path.abspath(\"LLM\")\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/matt/.llama/checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into `Transfomer` instance from source code\n",
    "\n",
    "We are interested in the activations through the layers of this model, so it would be good to load create an instance of the `Transformer` object defined in the `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.4545454545454546, norm_eps=1e-05, rope_theta=500000, use_scaled_rope=True, rope_scale_factor=32.0, max_batch_size=32, original_rotary_embed_len=8192, cache_len=2048)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from llama.model import ModelArgs\n",
    "\n",
    "config_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# extract the necessary fields\n",
    "model_args = ModelArgs(\n",
    "    dim=config.get(\"hidden_size\", 4096), \n",
    "    n_layers=config.get(\"num_hidden_layers\", 32),  \n",
    "    n_heads=config.get(\"num_attention_heads\", 32), \n",
    "    n_kv_heads=config.get(\"num_key_value_heads\", None), \n",
    "    vocab_size=config.get(\"vocab_size\", -1), \n",
    "    multiple_of=256, # not in config so use the default\n",
    "    norm_eps=config.get(\"rms_norm_eps\", 1e-5),  # map \"rms_norm_eps\"\n",
    "    max_batch_size=32,  # not in config so use the default\n",
    "    use_scale_rope=True, # this is how it was in llama.ipynb\n",
    "    rope_scale_factor=config.get(\"rope_scaling\").get(\"factor\"),\n",
    "    original_rotary_embed_len=config.get(\"rope_scaling\").get(\"original_max_position_embeddings\"),\n",
    "    cache_len = 2048,\n",
    "    ffn_dim_multiplier = 16 / 11\n",
    ")\n",
    "\n",
    "print(model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'attention_dropout': 0.0, 'bos_token_id': 128000, 'eos_token_id': 128001, 'head_dim': 64, 'hidden_act': 'silu', 'hidden_size': 2048, 'initializer_range': 0.02, 'intermediate_size': 8192, 'max_position_embeddings': 131072, 'mlp_bias': False, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 16, 'num_key_value_heads': 8, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'rope_theta': 500000.0, 'tie_word_embeddings': True, 'torch_dtype': 'bfloat16', 'transformers_version': '4.49.0', 'use_cache': True, 'vocab_size': 128256}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "attention block initialised\n",
      "orinal freqs: tensor([1.0000e+00, 6.6360e-01, 4.4037e-01, 2.9223e-01, 1.9392e-01, 1.2869e-01,\n",
      "        8.5397e-02, 5.6670e-02, 3.7606e-02, 2.4955e-02, 1.6560e-02, 1.0990e-02,\n",
      "        7.2927e-03, 4.8394e-03, 3.2114e-03, 2.1311e-03, 1.4142e-03, 9.3847e-04,\n",
      "        6.2277e-04, 4.1327e-04, 2.7425e-04, 1.8199e-04, 1.2077e-04, 8.0143e-05,\n",
      "        5.3183e-05, 3.5292e-05, 2.3420e-05, 1.5542e-05, 1.0313e-05, 6.8440e-06,\n",
      "        4.5417e-06, 3.0139e-06])\n",
      "rescaled freqs: tensor([1.0000e+00, 6.6360e-01, 4.4037e-01, 2.9223e-01, 1.9392e-01, 1.2869e-01,\n",
      "        8.5397e-02, 5.6670e-02, 3.7606e-02, 2.4955e-02, 1.6560e-02, 1.0990e-02,\n",
      "        7.2927e-03, 4.8394e-03, 3.2114e-03, 2.1311e-03, 1.4142e-03, 9.3847e-04,\n",
      "        1.9462e-05, 1.2915e-05, 8.5703e-06, 5.6872e-06, 3.7741e-06, 2.5045e-06,\n",
      "        1.6620e-06, 1.1029e-06, 7.3187e-07, 4.8567e-07, 3.2229e-07, 2.1387e-07,\n",
      "        1.4193e-07, 9.4183e-08])\n",
      "Transformer created\n",
      "weights in RAM\n"
     ]
    }
   ],
   "source": [
    "from llama.model import Transformer\n",
    "\n",
    "# I upgraded to 16Gb of RAM and now this will run - just need to tune the max sequence length as it will preallocate\n",
    "# the caches in the attention blocks based on that value\n",
    "\n",
    "# RAM preservation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.bfloat16 \n",
    "\n",
    "model = Transformer(model_args)\n",
    "\n",
    "#.to(dtype=torch_dtype)\n",
    "\n",
    "print(\"Transformer created\")\n",
    "\n",
    "safetensors_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf/model.safetensors\"  \n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "print(\"weights in RAM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the base frequencies for RoPE are the same as what we had in the llama.ipynb notebook; after we apply the scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights' names don't match what is in the code I found\n",
    "# so we rename them\n",
    "\n",
    "weights = load_file(safetensors_path)\n",
    "\n",
    "# create a new state dict with corrected names\n",
    "fixed_state_dict = {}\n",
    "\n",
    "for key in weights.keys():\n",
    "    \n",
    "    new_key = key\n",
    "    new_key = new_key.replace(\"model.\", \"\")\n",
    "    \n",
    "    new_key = new_key.replace(\"embed_tokens.weight\", \"tok_embeddings.weight\")\n",
    "\n",
    "    new_key = new_key.replace(\"self_attn.q_proj\", \"attention.wq\")\n",
    "    new_key = new_key.replace(\"self_attn.k_proj\", \"attention.wk\")\n",
    "    new_key = new_key.replace(\"self_attn.v_proj\", \"attention.wv\")\n",
    "    new_key = new_key.replace(\"self_attn.o_proj\", \"attention.wo\")\n",
    "\n",
    "    new_key = new_key.replace(\"mlp.gate_proj\", \"feed_forward.w1\")\n",
    "    new_key = new_key.replace(\"mlp.up_proj\", \"feed_forward.w3\")\n",
    "    new_key = new_key.replace(\"mlp.down_proj\", \"feed_forward.w2\")\n",
    "\n",
    "    new_key = new_key.replace(\"input_layernorm\", \"attention_norm\")\n",
    "    new_key = new_key.replace(\"post_attention_layernorm\", \"ffn_norm\")\n",
    "\n",
    "    new_key = new_key.replace(\"model.norm\", \"norm\")\n",
    "\n",
    "    fixed_state_dict[new_key] = weights[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tok_embeddings.weight', 'layers.0.attention_norm.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.ffn_norm.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wo.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wv.weight', 'layers.1.attention_norm.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.ffn_norm.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wo.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wv.weight', 'layers.10.attention_norm.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.ffn_norm.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wo.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wv.weight', 'layers.11.attention_norm.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.ffn_norm.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wo.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wv.weight', 'layers.12.attention_norm.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.ffn_norm.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wo.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wv.weight', 'layers.13.attention_norm.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.ffn_norm.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wo.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wv.weight', 'layers.14.attention_norm.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.ffn_norm.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wo.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wv.weight', 'layers.15.attention_norm.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.ffn_norm.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wo.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wv.weight', 'layers.2.attention_norm.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.ffn_norm.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wo.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wv.weight', 'layers.3.attention_norm.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.ffn_norm.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wo.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wv.weight', 'layers.4.attention_norm.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.ffn_norm.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wo.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wv.weight', 'layers.5.attention_norm.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.ffn_norm.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wo.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wv.weight', 'layers.6.attention_norm.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.ffn_norm.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wo.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wv.weight', 'layers.7.attention_norm.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.ffn_norm.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wo.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wv.weight', 'layers.8.attention_norm.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.ffn_norm.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wo.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wv.weight', 'layers.9.attention_norm.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.ffn_norm.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wo.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wv.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with corrected state_dict\n"
     ]
    }
   ],
   "source": [
    "# Load the corrected state dict\n",
    "model.load_state_dict(fixed_state_dict, strict = False) # since the output weights are tied, these are already correct and not loaded - but the Transformer class expects the; so load non-strictly\n",
    "\n",
    "print(\"loaded model with corrected state_dict\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): ColumnParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "        (wk): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wv): ColumnParallelLinear(in_features=2048, out_features=512, bias=False)\n",
       "        (wo): RowParallelLinear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): RowParallelLinear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): ColumnParallelLinear(in_features=2048, out_features=8192, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): ColumnParallelLinear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tie the output embedding manually\n",
    "\n",
    "model.output.weight.data = model.tok_embeddings.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.tokenizer.Tokenizer at 0x7f4418a2cbc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "# go here to find the model file\n",
    "# https://github.com/meta-llama/llama-models/blob/main/models/llama3/api/tokenizer.model (689c7f2)\n",
    "\n",
    "new_tok_path = \"/home/matt/.llama/checkpoints/Llama3.2-1B-hf-tok/tokenizer.model\"\n",
    "new_tok = Tokenizer(new_tok_path)\n",
    "new_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama\n",
    "\n",
    "llama = Llama(model, new_tok, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama.generation.Llama at 0x7f41b0fa4560>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInput to model:\n",
      "<|begin_of_text|>The capital of France is\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama.generation import CompletionPrediction\n",
    "\n",
    "# A high top_p seems necessary here, otherwise we just get strings of numbers\n",
    "res: CompletionPrediction = llama.text_completion(\"Hello how are you?\", max_gen_len=50, top_p = 0.9, temperature = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Paris. This city has a population about 2,000 people.\\nThis place was built in the year and it's Capital of The country France\\nFrance region Of Region Map - For more than million inhabitants which live on river bank: On beautiful area\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.078843116760254],\n",
       " [-1.2045153379440308],\n",
       " [-2.747781991958618],\n",
       " [-1.3441989421844482],\n",
       " [-1.9368460178375244],\n",
       " [-1.20782470703125],\n",
       " [-1.1919770240783691],\n",
       " [-3.838620901107788],\n",
       " [-0.1342192143201828],\n",
       " [-0.33271750807762146],\n",
       " [-1.1273064613342285],\n",
       " [-2.4497766494750977],\n",
       " [-2.324704170227051],\n",
       " [-2.197195053100586],\n",
       " [-3.1740481853485107],\n",
       " [-4.065153121948242],\n",
       " [-2.3178908824920654],\n",
       " [-2.6613805294036865],\n",
       " [-0.8573299050331116],\n",
       " [-1.07711660861969],\n",
       " [-1.4414087533950806],\n",
       " [-4.049238681793213],\n",
       " [-2.1237897872924805],\n",
       " [-0.6591014862060547],\n",
       " [-6.1876020431518555],\n",
       " [-0.6143554449081421],\n",
       " [-5.429050445556641],\n",
       " [-2.6682536602020264],\n",
       " [-1.0977528095245361],\n",
       " [-3.2166032791137695],\n",
       " [-2.9457433223724365],\n",
       " [-6.2648606300354],\n",
       " [-6.69734001159668],\n",
       " [-5.850415229797363],\n",
       " [-6.930875778198242],\n",
       " [-3.9093780517578125],\n",
       " [-6.449533462524414],\n",
       " [-3.8352251052856445],\n",
       " [-2.0982353687286377],\n",
       " [-3.828277826309204],\n",
       " [-2.7142529487609863],\n",
       " [-5.48582649230957],\n",
       " [-4.274913787841797],\n",
       " [-3.610187292098999],\n",
       " [-7.085535049438477],\n",
       " [-3.537109613418579],\n",
       " [-6.62539005279541],\n",
       " [-5.371893405914307],\n",
       " [-7.478360652923584],\n",
       " [-5.324193477630615]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordSenseDisambiguation",
   "language": "python",
   "name": "wordsensedisambiguation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
